--- /dev/null
+++ b/toolchain/musl/patches/902-glibc-aarch64-asm.patch
@@ -0,0 +1,845 @@
+--- a/src/string/aarch64/memset.S	2024-09-15 14:42:42.293938366 +0800
++++ b/src/string/aarch64/memset.S	2024-09-15 14:42:44.476821512 +0800
+@@ -1,8 +1,8 @@
+ /*
+  * memset - fill memory with a constant byte
+  *
+- * Copyright (c) 2012-2020, Arm Limited.
+- * SPDX-License-Identifier: MIT
++ * Copyright (c) 2012-2024, Arm Limited.
++ * SPDX-License-Identifier: MIT OR Apache-2.0 WITH LLVM-exception
+  */
+ 
+ /* Assumptions:
+@@ -11,105 +11,110 @@
+  *
+  */
+ 
+-#define dstin   x0
+-#define val     x1
+-#define valw    w1
+-#define count   x2
+-#define dst     x3
+-#define dstend  x4
+-#define zva_val x5
++#define dstin	x0
++#define val	x1
++#define valw	w1
++#define count	x2
++#define dst	x3
++#define dstend	x4
++#define zva_val	x5
++#define off	x3
++#define dstend2	x5
+ 
+ .global memset
+ .type memset,%function
+ memset:
++	dup	v0.16B, valw
++	cmp	count, 16
++	b.lo	.Lset_small
++
++	add	dstend, dstin, count
++	cmp	count, 64
++	b.hs	.Lset_128
++
++	/* Set 16..63 bytes.  */
++	mov	off, 16
++	and	off, off, count, lsr 1
++	sub	dstend2, dstend, off
++	str	q0, [dstin]
++	str	q0, [dstin, off]
++	str	q0, [dstend2, -16]
++	str	q0, [dstend, -16]
++	ret
+ 
+-	dup     v0.16B, valw
+-	add     dstend, dstin, count
+-
+-	cmp     count, 96
+-	b.hi    .Lset_long
+-	cmp     count, 16
+-	b.hs    .Lset_medium
+-	mov     val, v0.D[0]
+-
++	.p2align 4
+ 	/* Set 0..15 bytes.  */
+-	tbz     count, 3, 1f
+-	str     val, [dstin]
+-	str     val, [dstend, -8]
++.Lset_small:
++	add	dstend, dstin, count
++	cmp	count, 4
++	b.lo	2f
++	lsr	off, count, 3
++	sub	dstend2, dstend, off, lsl 2
++	str	s0, [dstin]
++	str	s0, [dstin, off, lsl 2]
++	str	s0, [dstend2, -4]
++	str	s0, [dstend, -4]
+ 	ret
+-	nop
+-1:      tbz     count, 2, 2f
+-	str     valw, [dstin]
+-	str     valw, [dstend, -4]
+-	ret
+-2:      cbz     count, 3f
+-	strb    valw, [dstin]
+-	tbz     count, 1, 3f
+-	strh    valw, [dstend, -2]
+-3:      ret
+-
+-	/* Set 17..96 bytes.  */
+-.Lset_medium:
+-	str     q0, [dstin]
+-	tbnz    count, 6, .Lset96
+-	str     q0, [dstend, -16]
+-	tbz     count, 5, 1f
+-	str     q0, [dstin, 16]
+-	str     q0, [dstend, -32]
+-1:      ret
++
++	/* Set 0..3 bytes.  */
++2:	cbz	count, 3f
++	lsr	off, count, 1
++	strb	valw, [dstin]
++	strb	valw, [dstin, off]
++	strb	valw, [dstend, -1]
++3:	ret
+ 
+ 	.p2align 4
+-	/* Set 64..96 bytes.  Write 64 bytes from the start and
+-	   32 bytes from the end.  */
+-.Lset96:
+-	str     q0, [dstin, 16]
+-	stp     q0, q0, [dstin, 32]
+-	stp     q0, q0, [dstend, -32]
++.Lset_128:
++	bic	dst, dstin, 15
++	cmp	count, 128
++	b.hi	.Lset_long
++	stp	q0, q0, [dstin]
++	stp	q0, q0, [dstin, 32]
++	stp	q0, q0, [dstend, -64]
++	stp	q0, q0, [dstend, -32]
+ 	ret
+ 
+ 	.p2align 4
+ .Lset_long:
+-	and     valw, valw, 255
+-	bic     dst, dstin, 15
+-	str     q0, [dstin]
+-	cmp     count, 160
+-	ccmp    valw, 0, 0, hs
+-	b.ne    .Lno_zva
+-
++	str	q0, [dstin]
++	str	q0, [dst, 16]
++	tst	valw, 255
++	b.ne	.Lno_zva
+ #ifndef SKIP_ZVA_CHECK
+-	mrs     zva_val, dczid_el0
+-	and     zva_val, zva_val, 31
+-	cmp     zva_val, 4              /* ZVA size is 64 bytes.  */
+-	b.ne    .Lno_zva
++	mrs	zva_val, dczid_el0
++	and	zva_val, zva_val, 31
++	cmp	zva_val, 4		/* ZVA size is 64 bytes.  */
++	b.ne	.Lno_zva
+ #endif
+-	str     q0, [dst, 16]
+-	stp     q0, q0, [dst, 32]
+-	bic     dst, dst, 63
+-	sub     count, dstend, dst      /* Count is now 64 too large.  */
+-	sub     count, count, 128       /* Adjust count and bias for loop.  */
++	stp	q0, q0, [dst, 32]
++	bic	dst, dstin, 63
++	sub	count, dstend, dst	/* Count is now 64 too large.  */
++	sub	count, count, 64 + 64	/* Adjust count and bias for loop.  */
++
++	/* Write last bytes before ZVA loop.  */
++	stp	q0, q0, [dstend, -64]
++	stp	q0, q0, [dstend, -32]
+ 
+ 	.p2align 4
+-.Lzva_loop:
+-	add     dst, dst, 64
+-	dc      zva, dst
+-	subs    count, count, 64
+-	b.hi    .Lzva_loop
+-	stp     q0, q0, [dstend, -64]
+-	stp     q0, q0, [dstend, -32]
++.Lzva64_loop:
++	add	dst, dst, 64
++	dc	zva, dst
++	subs	count, count, 64
++	b.hi	.Lzva64_loop
+ 	ret
+ 
++	.p2align 3
+ .Lno_zva:
+-	sub     count, dstend, dst      /* Count is 16 too large.  */
+-	sub     dst, dst, 16            /* Dst is biased by -32.  */
+-	sub     count, count, 64 + 16   /* Adjust count and bias for loop.  */
++	sub	count, dstend, dst	/* Count is 32 too large.  */
++	sub	count, count, 64 + 32	/* Adjust count and bias for loop.  */
+ .Lno_zva_loop:
+-	stp     q0, q0, [dst, 32]
+-	stp     q0, q0, [dst, 64]!
+-	subs    count, count, 64
+-	b.hi    .Lno_zva_loop
+-	stp     q0, q0, [dstend, -64]
+-	stp     q0, q0, [dstend, -32]
++	stp	q0, q0, [dst, 32]
++	stp	q0, q0, [dst, 64]
++	add	dst, dst, 64
++	subs	count, count, 64
++	b.hi	.Lno_zva_loop
++	stp	q0, q0, [dstend, -64]
++	stp	q0, q0, [dstend, -32]
+ 	ret
+ 
+-.size memset,.-memset
+-
+--- a/src/string/aarch64/memcpy.S	2024-03-01 10:07:33.000000000 +0800
++++ b/src/string/aarch64/memcpy.S	2024-09-14 15:55:43.209627697 +0800
+@@ -1,186 +1,238 @@
+-/*
+- * memcpy - copy memory area
+- *
+- * Copyright (c) 2012-2020, Arm Limited.
+- * SPDX-License-Identifier: MIT
+- */
++/* Generic optimized memcpy using SIMD.
++   Copyright (C) 2012-2024 Free Software Foundation, Inc.
++
++   This file is part of the GNU C Library.
++
++   The GNU C Library is free software; you can redistribute it and/or
++   modify it under the terms of the GNU Lesser General Public
++   License as published by the Free Software Foundation; either
++   version 2.1 of the License, or (at your option) any later version.
++
++   The GNU C Library is distributed in the hope that it will be useful,
++   but WITHOUT ANY WARRANTY; without even the implied warranty of
++   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++   Lesser General Public License for more details.
++
++   You should have received a copy of the GNU Lesser General Public
++   License along with the GNU C Library.  If not, see
++   <https://www.gnu.org/licenses/>.  */
+ 
+ /* Assumptions:
+  *
+- * ARMv8-a, AArch64, unaligned accesses.
++ * ARMv8-a, AArch64, Advanced SIMD, unaligned accesses.
+  *
+  */
+ 
+-#define dstin   x0
+-#define src     x1
+-#define count   x2
+-#define dst     x3
+-#define srcend  x4
+-#define dstend  x5
+-#define A_l     x6
+-#define A_lw    w6
+-#define A_h     x7
+-#define B_l     x8
+-#define B_lw    w8
+-#define B_h     x9
+-#define C_l     x10
+-#define C_lw    w10
+-#define C_h     x11
+-#define D_l     x12
+-#define D_h     x13
+-#define E_l     x14
+-#define E_h     x15
+-#define F_l     x16
+-#define F_h     x17
+-#define G_l     count
+-#define G_h     dst
+-#define H_l     src
+-#define H_h     srcend
+-#define tmp1    x14
+-
+-/* This implementation of memcpy uses unaligned accesses and branchless
+-   sequences to keep the code small, simple and improve performance.
++#define dstin	x0
++#define src	x1
++#define count	x2
++#define dst	x3
++#define srcend	x4
++#define dstend	x5
++#define A_l	x6
++#define A_lw	w6
++#define A_h	x7
++#define B_l	x8
++#define B_lw	w8
++#define B_h	x9
++#define C_lw	w10
++#define tmp1	x14
++
++#define A_q	q0
++#define B_q	q1
++#define C_q	q2
++#define D_q	q3
++#define E_q	q4
++#define F_q	q5
++#define G_q	q6
++#define H_q	q7
++
++/* This implementation supports both memcpy and memmove and shares most code.
++   It uses unaligned accesses and branchless sequences to keep the code small,
++   simple and improve performance.
+ 
+    Copies are split into 3 main cases: small copies of up to 32 bytes, medium
+    copies of up to 128 bytes, and large copies.  The overhead of the overlap
+-   check is negligible since it is only required for large copies.
++   check in memmove is negligible since it is only required for large copies.
+ 
+-   Large copies use a software pipelined loop processing 64 bytes per iteration.
+-   The destination pointer is 16-byte aligned to minimize unaligned accesses.
+-   The loop tail is handled by always copying 64 bytes from the end.
+-*/
++   Large copies use a software pipelined loop processing 64 bytes per
++   iteration.  The destination pointer is 16-byte aligned to minimize
++   unaligned accesses.  The loop tail is handled by always copying 64 bytes
++   from the end.  */
+ 
+ .global memcpy
++.global memmove
++
+ .type memcpy,%function
++.type memmove,%function
++
+ memcpy:
+-	add     srcend, src, count
+-	add     dstend, dstin, count
+-	cmp     count, 128
+-	b.hi    .Lcopy_long
+-	cmp     count, 32
+-	b.hi    .Lcopy32_128
++	add	srcend, src, count
++	add	dstend, dstin, count
++	cmp	count, 128
++	b.hi	.Lcopy_long
++	cmp	count, 32
++	b.hi	.Lcopy32_128
+ 
+ 	/* Small copies: 0..32 bytes.  */
+-	cmp     count, 16
+-	b.lo    .Lcopy16
+-	ldp     A_l, A_h, [src]
+-	ldp     D_l, D_h, [srcend, -16]
+-	stp     A_l, A_h, [dstin]
+-	stp     D_l, D_h, [dstend, -16]
++	cmp	count, 16
++	b.lo	.Lcopy16
++	ldr	A_q, [src]
++	ldr	B_q, [srcend, -16]
++	str	A_q, [dstin]
++	str	B_q, [dstend, -16]
+ 	ret
+ 
+ 	/* Copy 8-15 bytes.  */
+ .Lcopy16:
+-	tbz     count, 3, .Lcopy8
+-	ldr     A_l, [src]
+-	ldr     A_h, [srcend, -8]
+-	str     A_l, [dstin]
+-	str     A_h, [dstend, -8]
++	tbz	count, 3, .Lcopy8
++	ldr	A_l, [src]
++	ldr	A_h, [srcend, -8]
++	str	A_l, [dstin]
++	str	A_h, [dstend, -8]
+ 	ret
+ 
+-	.p2align 3
+ 	/* Copy 4-7 bytes.  */
+ .Lcopy8:
+-	tbz     count, 2, .Lcopy4
+-	ldr     A_lw, [src]
+-	ldr     B_lw, [srcend, -4]
+-	str     A_lw, [dstin]
+-	str     B_lw, [dstend, -4]
++	tbz	count, 2, .Lcopy4
++	ldr	A_lw, [src]
++	ldr	B_lw, [srcend, -4]
++	str	A_lw, [dstin]
++	str	B_lw, [dstend, -4]
+ 	ret
+ 
+ 	/* Copy 0..3 bytes using a branchless sequence.  */
+ .Lcopy4:
+-	cbz     count, .Lcopy0
+-	lsr     tmp1, count, 1
+-	ldrb    A_lw, [src]
+-	ldrb    C_lw, [srcend, -1]
+-	ldrb    B_lw, [src, tmp1]
+-	strb    A_lw, [dstin]
+-	strb    B_lw, [dstin, tmp1]
+-	strb    C_lw, [dstend, -1]
++	cbz	count, .Lcopy0
++	lsr	tmp1, count, 1
++	ldrb	A_lw, [src]
++	ldrb	C_lw, [srcend, -1]
++	ldrb	B_lw, [src, tmp1]
++	strb	A_lw, [dstin]
++	strb	B_lw, [dstin, tmp1]
++	strb	C_lw, [dstend, -1]
+ .Lcopy0:
+ 	ret
+ 
+ 	.p2align 4
+ 	/* Medium copies: 33..128 bytes.  */
+ .Lcopy32_128:
+-	ldp     A_l, A_h, [src]
+-	ldp     B_l, B_h, [src, 16]
+-	ldp     C_l, C_h, [srcend, -32]
+-	ldp     D_l, D_h, [srcend, -16]
+-	cmp     count, 64
+-	b.hi    .Lcopy128
+-	stp     A_l, A_h, [dstin]
+-	stp     B_l, B_h, [dstin, 16]
+-	stp     C_l, C_h, [dstend, -32]
+-	stp     D_l, D_h, [dstend, -16]
++	ldp	A_q, B_q, [src]
++	ldp	C_q, D_q, [srcend, -32]
++	cmp	count, 64
++	b.hi	.Lcopy128
++	stp	A_q, B_q, [dstin]
++	stp	C_q, D_q, [dstend, -32]
+ 	ret
+ 
+ 	.p2align 4
+ 	/* Copy 65..128 bytes.  */
+ .Lcopy128:
+-	ldp     E_l, E_h, [src, 32]
+-	ldp     F_l, F_h, [src, 48]
+-	cmp     count, 96
+-	b.ls    .Lcopy96
+-	ldp     G_l, G_h, [srcend, -64]
+-	ldp     H_l, H_h, [srcend, -48]
+-	stp     G_l, G_h, [dstend, -64]
+-	stp     H_l, H_h, [dstend, -48]
++	ldp	E_q, F_q, [src, 32]
++	cmp	count, 96
++	b.ls	.Lcopy96
++	ldp	G_q, H_q, [srcend, -64]
++	stp	G_q, H_q, [dstend, -64]
+ .Lcopy96:
+-	stp     A_l, A_h, [dstin]
+-	stp     B_l, B_h, [dstin, 16]
+-	stp     E_l, E_h, [dstin, 32]
+-	stp     F_l, F_h, [dstin, 48]
+-	stp     C_l, C_h, [dstend, -32]
+-	stp     D_l, D_h, [dstend, -16]
++	stp	A_q, B_q, [dstin]
++	stp	E_q, F_q, [dstin, 32]
++	stp	C_q, D_q, [dstend, -32]
+ 	ret
+ 
+-	.p2align 4
++	/* Align loop64 below to 16 bytes.  */
++	nop
++
+ 	/* Copy more than 128 bytes.  */
+ .Lcopy_long:
+-
+-	/* Copy 16 bytes and then align dst to 16-byte alignment.  */
+-
+-	ldp     D_l, D_h, [src]
+-	and     tmp1, dstin, 15
+-	bic     dst, dstin, 15
+-	sub     src, src, tmp1
+-	add     count, count, tmp1      /* Count is now 16 too large.  */
+-	ldp     A_l, A_h, [src, 16]
+-	stp     D_l, D_h, [dstin]
+-	ldp     B_l, B_h, [src, 32]
+-	ldp     C_l, C_h, [src, 48]
+-	ldp     D_l, D_h, [src, 64]!
+-	subs    count, count, 128 + 16  /* Test and readjust count.  */
+-	b.ls    .Lcopy64_from_end
+-
++	/* Copy 16 bytes and then align src to 16-byte alignment.  */
++	ldr	D_q, [src]
++	and	tmp1, src, 15
++	bic	src, src, 15
++	sub	dst, dstin, tmp1
++	add	count, count, tmp1	/* Count is now 16 too large.  */
++	ldp	A_q, B_q, [src, 16]
++	str	D_q, [dstin]
++	ldp	C_q, D_q, [src, 48]
++	subs	count, count, 128 + 16	/* Test and readjust count.  */
++	b.ls	.Lcopy64_from_end
+ .Lloop64:
+-	stp     A_l, A_h, [dst, 16]
+-	ldp     A_l, A_h, [src, 16]
+-	stp     B_l, B_h, [dst, 32]
+-	ldp     B_l, B_h, [src, 32]
+-	stp     C_l, C_h, [dst, 48]
+-	ldp     C_l, C_h, [src, 48]
+-	stp     D_l, D_h, [dst, 64]!
+-	ldp     D_l, D_h, [src, 64]!
+-	subs    count, count, 64
+-	b.hi    .Lloop64
++	stp	A_q, B_q, [dst, 16]
++	ldp	A_q, B_q, [src, 80]
++	stp	C_q, D_q, [dst, 48]
++	ldp	C_q, D_q, [src, 112]
++	add	src, src, 64
++	add	dst, dst, 64
++	subs	count, count, 64
++	b.hi	.Lloop64
+ 
+ 	/* Write the last iteration and copy 64 bytes from the end.  */
+ .Lcopy64_from_end:
+-	ldp     E_l, E_h, [srcend, -64]
+-	stp     A_l, A_h, [dst, 16]
+-	ldp     A_l, A_h, [srcend, -48]
+-	stp     B_l, B_h, [dst, 32]
+-	ldp     B_l, B_h, [srcend, -32]
+-	stp     C_l, C_h, [dst, 48]
+-	ldp     C_l, C_h, [srcend, -16]
+-	stp     D_l, D_h, [dst, 64]
+-	stp     E_l, E_h, [dstend, -64]
+-	stp     A_l, A_h, [dstend, -48]
+-	stp     B_l, B_h, [dstend, -32]
+-	stp     C_l, C_h, [dstend, -16]
++	ldp	E_q, F_q, [srcend, -64]
++	stp	A_q, B_q, [dst, 16]
++	ldp	A_q, B_q, [srcend, -32]
++	stp	C_q, D_q, [dst, 48]
++	stp	E_q, F_q, [dstend, -64]
++	stp	A_q, B_q, [dstend, -32]
++	ret
++
++memmove:
++
++	add	srcend, src, count
++	add	dstend, dstin, count
++	cmp	count, 128
++	b.hi	.Lmove_long
++	cmp	count, 32
++	b.hi	.Lcopy32_128
++
++	/* Small moves: 0..32 bytes.  */
++	cmp	count, 16
++	b.lo	.Lcopy16
++	ldr	A_q, [src]
++	ldr	B_q, [srcend, -16]
++	str	A_q, [dstin]
++	str	B_q, [dstend, -16]
++	ret
++
++.Lmove_long:
++	/* Only use backward copy if there is an overlap.  */
++	sub	tmp1, dstin, src
++	cbz	tmp1, .Lmove0
++	cmp	tmp1, count
++	b.hs	.Lcopy_long
++
++	/* Large backwards copy for overlapping copies.
++	   Copy 16 bytes and then align srcend to 16-byte alignment.  */
++.Lcopy_long_backwards:
++	ldr	D_q, [srcend, -16]
++	and	tmp1, srcend, 15
++	bic	srcend, srcend, 15
++	sub	count, count, tmp1
++	ldp	A_q, B_q, [srcend, -32]
++	str	D_q, [dstend, -16]
++	ldp	C_q, D_q, [srcend, -64]
++	sub	dstend, dstend, tmp1
++	subs	count, count, 128
++	b.ls	.Lcopy64_from_start
++
++.Lloop64_backwards:
++	str	B_q, [dstend, -16]
++	str	A_q, [dstend, -32]
++	ldp	A_q, B_q, [srcend, -96]
++	str	D_q, [dstend, -48]
++	str	C_q, [dstend, -64]!
++	ldp	C_q, D_q, [srcend, -128]
++	sub	srcend, srcend, 64
++	subs	count, count, 64
++	b.hi	.Lloop64_backwards
++
++	/* Write the last iteration and copy 64 bytes from the start.  */
++.Lcopy64_from_start:
++	ldp	E_q, F_q, [src, 32]
++	stp	A_q, B_q, [dstend, -32]
++	ldp	A_q, B_q, [src]
++	stp	C_q, D_q, [dstend, -64]
++	stp	E_q, F_q, [dstin, 32]
++	stp	A_q, B_q, [dstin]
++.Lmove0:
+ 	ret
+ 
+-.size memcpy,.-memcpy
+--- a/src/string/memmove.c	2024-03-01 10:07:33.000000000 +0800
++++ b/dev/null	2024-09-12 13:27:00.930999908 +0800
+@@ -1,42 +0,0 @@
+-#include <string.h>
+-#include <stdint.h>
+-
+-#ifdef __GNUC__
+-typedef __attribute__((__may_alias__)) size_t WT;
+-#define WS (sizeof(WT))
+-#endif
+-
+-void *memmove(void *dest, const void *src, size_t n)
+-{
+-	char *d = dest;
+-	const char *s = src;
+-
+-	if (d==s) return d;
+-	if ((uintptr_t)s-(uintptr_t)d-n <= -2*n) return memcpy(d, s, n);
+-
+-	if (d<s) {
+-#ifdef __GNUC__
+-		if ((uintptr_t)s % WS == (uintptr_t)d % WS) {
+-			while ((uintptr_t)d % WS) {
+-				if (!n--) return dest;
+-				*d++ = *s++;
+-			}
+-			for (; n>=WS; n-=WS, d+=WS, s+=WS) *(WT *)d = *(WT *)s;
+-		}
+-#endif
+-		for (; n; n--) *d++ = *s++;
+-	} else {
+-#ifdef __GNUC__
+-		if ((uintptr_t)s % WS == (uintptr_t)d % WS) {
+-			while ((uintptr_t)(d+n) % WS) {
+-				if (!n--) return dest;
+-				d[n] = s[n];
+-			}
+-			while (n>=WS) n-=WS, *(WT *)(d+n) = *(WT *)(s+n);
+-		}
+-#endif
+-		while (n) n--, d[n] = s[n];
+-	}
+-
+-	return dest;
+-}
+--- a/src/string/memcpy.c	2024-03-01 10:07:33.000000000 +0800
++++ b/dev/null	2024-10-05 22:57:56.162999993 +0800
+@@ -1,124 +0,0 @@
+-#include <string.h>
+-#include <stdint.h>
+-#include <endian.h>
+-
+-void *memcpy(void *restrict dest, const void *restrict src, size_t n)
+-{
+-	unsigned char *d = dest;
+-	const unsigned char *s = src;
+-
+-#ifdef __GNUC__
+-
+-#if __BYTE_ORDER == __LITTLE_ENDIAN
+-#define LS >>
+-#define RS <<
+-#else
+-#define LS <<
+-#define RS >>
+-#endif
+-
+-	typedef uint32_t __attribute__((__may_alias__)) u32;
+-	uint32_t w, x;
+-
+-	for (; (uintptr_t)s % 4 && n; n--) *d++ = *s++;
+-
+-	if ((uintptr_t)d % 4 == 0) {
+-		for (; n>=16; s+=16, d+=16, n-=16) {
+-			*(u32 *)(d+0) = *(u32 *)(s+0);
+-			*(u32 *)(d+4) = *(u32 *)(s+4);
+-			*(u32 *)(d+8) = *(u32 *)(s+8);
+-			*(u32 *)(d+12) = *(u32 *)(s+12);
+-		}
+-		if (n&8) {
+-			*(u32 *)(d+0) = *(u32 *)(s+0);
+-			*(u32 *)(d+4) = *(u32 *)(s+4);
+-			d += 8; s += 8;
+-		}
+-		if (n&4) {
+-			*(u32 *)(d+0) = *(u32 *)(s+0);
+-			d += 4; s += 4;
+-		}
+-		if (n&2) {
+-			*d++ = *s++; *d++ = *s++;
+-		}
+-		if (n&1) {
+-			*d = *s;
+-		}
+-		return dest;
+-	}
+-
+-	if (n >= 32) switch ((uintptr_t)d % 4) {
+-	case 1:
+-		w = *(u32 *)s;
+-		*d++ = *s++;
+-		*d++ = *s++;
+-		*d++ = *s++;
+-		n -= 3;
+-		for (; n>=17; s+=16, d+=16, n-=16) {
+-			x = *(u32 *)(s+1);
+-			*(u32 *)(d+0) = (w LS 24) | (x RS 8);
+-			w = *(u32 *)(s+5);
+-			*(u32 *)(d+4) = (x LS 24) | (w RS 8);
+-			x = *(u32 *)(s+9);
+-			*(u32 *)(d+8) = (w LS 24) | (x RS 8);
+-			w = *(u32 *)(s+13);
+-			*(u32 *)(d+12) = (x LS 24) | (w RS 8);
+-		}
+-		break;
+-	case 2:
+-		w = *(u32 *)s;
+-		*d++ = *s++;
+-		*d++ = *s++;
+-		n -= 2;
+-		for (; n>=18; s+=16, d+=16, n-=16) {
+-			x = *(u32 *)(s+2);
+-			*(u32 *)(d+0) = (w LS 16) | (x RS 16);
+-			w = *(u32 *)(s+6);
+-			*(u32 *)(d+4) = (x LS 16) | (w RS 16);
+-			x = *(u32 *)(s+10);
+-			*(u32 *)(d+8) = (w LS 16) | (x RS 16);
+-			w = *(u32 *)(s+14);
+-			*(u32 *)(d+12) = (x LS 16) | (w RS 16);
+-		}
+-		break;
+-	case 3:
+-		w = *(u32 *)s;
+-		*d++ = *s++;
+-		n -= 1;
+-		for (; n>=19; s+=16, d+=16, n-=16) {
+-			x = *(u32 *)(s+3);
+-			*(u32 *)(d+0) = (w LS 8) | (x RS 24);
+-			w = *(u32 *)(s+7);
+-			*(u32 *)(d+4) = (x LS 8) | (w RS 24);
+-			x = *(u32 *)(s+11);
+-			*(u32 *)(d+8) = (w LS 8) | (x RS 24);
+-			w = *(u32 *)(s+15);
+-			*(u32 *)(d+12) = (x LS 8) | (w RS 24);
+-		}
+-		break;
+-	}
+-	if (n&16) {
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-	}
+-	if (n&8) {
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-	}
+-	if (n&4) {
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-	}
+-	if (n&2) {
+-		*d++ = *s++; *d++ = *s++;
+-	}
+-	if (n&1) {
+-		*d = *s;
+-	}
+-	return dest;
+-#endif
+-
+-	for (; n; n--) *d++ = *s++;
+-	return dest;
+-}
+--- a/src/string/memset.c	2024-03-01 10:07:33.000000000 +0800
++++ b/dev/null	2024-10-05 22:57:56.162999993 +0800
+@@ -1,90 +0,0 @@
+-#include <string.h>
+-#include <stdint.h>
+-
+-void *memset(void *dest, int c, size_t n)
+-{
+-	unsigned char *s = dest;
+-	size_t k;
+-
+-	/* Fill head and tail with minimal branching. Each
+-	 * conditional ensures that all the subsequently used
+-	 * offsets are well-defined and in the dest region. */
+-
+-	if (!n) return dest;
+-	s[0] = c;
+-	s[n-1] = c;
+-	if (n <= 2) return dest;
+-	s[1] = c;
+-	s[2] = c;
+-	s[n-2] = c;
+-	s[n-3] = c;
+-	if (n <= 6) return dest;
+-	s[3] = c;
+-	s[n-4] = c;
+-	if (n <= 8) return dest;
+-
+-	/* Advance pointer to align it at a 4-byte boundary,
+-	 * and truncate n to a multiple of 4. The previous code
+-	 * already took care of any head/tail that get cut off
+-	 * by the alignment. */
+-
+-	k = -(uintptr_t)s & 3;
+-	s += k;
+-	n -= k;
+-	n &= -4;
+-
+-#ifdef __GNUC__
+-	typedef uint32_t __attribute__((__may_alias__)) u32;
+-	typedef uint64_t __attribute__((__may_alias__)) u64;
+-
+-	u32 c32 = ((u32)-1)/255 * (unsigned char)c;
+-
+-	/* In preparation to copy 32 bytes at a time, aligned on
+-	 * an 8-byte bounary, fill head/tail up to 28 bytes each.
+-	 * As in the initial byte-based head/tail fill, each
+-	 * conditional below ensures that the subsequent offsets
+-	 * are valid (e.g. !(n<=24) implies n>=28). */
+-
+-	*(u32 *)(s+0) = c32;
+-	*(u32 *)(s+n-4) = c32;
+-	if (n <= 8) return dest;
+-	*(u32 *)(s+4) = c32;
+-	*(u32 *)(s+8) = c32;
+-	*(u32 *)(s+n-12) = c32;
+-	*(u32 *)(s+n-8) = c32;
+-	if (n <= 24) return dest;
+-	*(u32 *)(s+12) = c32;
+-	*(u32 *)(s+16) = c32;
+-	*(u32 *)(s+20) = c32;
+-	*(u32 *)(s+24) = c32;
+-	*(u32 *)(s+n-28) = c32;
+-	*(u32 *)(s+n-24) = c32;
+-	*(u32 *)(s+n-20) = c32;
+-	*(u32 *)(s+n-16) = c32;
+-
+-	/* Align to a multiple of 8 so we can fill 64 bits at a time,
+-	 * and avoid writing the same bytes twice as much as is
+-	 * practical without introducing additional branching. */
+-
+-	k = 24 + ((uintptr_t)s & 4);
+-	s += k;
+-	n -= k;
+-
+-	/* If this loop is reached, 28 tail bytes have already been
+-	 * filled, so any remainder when n drops below 32 can be
+-	 * safely ignored. */
+-
+-	u64 c64 = c32 | ((u64)c32 << 32);
+-	for (; n >= 32; n-=32, s+=32) {
+-		*(u64 *)(s+0) = c64;
+-		*(u64 *)(s+8) = c64;
+-		*(u64 *)(s+16) = c64;
+-		*(u64 *)(s+24) = c64;
+-	}
+-#else
+-	/* Pure C fallback with no aliasing violations. */
+-	for (; n; n--, s++) *s = c;
+-#endif
+-
+-	return dest;
+-}
