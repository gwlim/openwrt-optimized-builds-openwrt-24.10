diff --git a/toolchain/musl/patches/906-musl-asm-optimize-arm.patch b/toolchain/musl/patches/906-musl-asm-optimize-arm.patch
new file mode 100644
index 0000000..27c83b5
--- /dev/null
+++ b/toolchain/musl/patches/906-musl-asm-optimize-arm.patch
@@ -0,0 +1,1956 @@
+--- a/src/string/arm/__aeabi_memcpy.s	2024-03-01 10:07:33.000000000 +0800
++++ /dev/null	2024-10-19 22:20:54.374000100 +0800
+@@ -1,45 +0,0 @@
+-.syntax unified
+-
+-.global __aeabi_memcpy8
+-.global __aeabi_memcpy4
+-.global __aeabi_memcpy
+-.global __aeabi_memmove8
+-.global __aeabi_memmove4
+-.global __aeabi_memmove
+-
+-.type __aeabi_memcpy8,%function
+-.type __aeabi_memcpy4,%function
+-.type __aeabi_memcpy,%function
+-.type __aeabi_memmove8,%function
+-.type __aeabi_memmove4,%function
+-.type __aeabi_memmove,%function
+-
+-__aeabi_memmove8:
+-__aeabi_memmove4:
+-__aeabi_memmove:
+-	cmp   r0, r1
+-	bls   3f
+-	cmp   r2, #0
+-	beq   2f
+-	adds  r0, r0, r2
+-	adds  r2, r1, r2
+-1:	subs  r2, r2, #1
+-	ldrb  r3, [r2]
+-	subs  r0, r0, #1
+-	strb  r3, [r0]
+-	cmp   r1, r2
+-	bne   1b
+-2:	bx    lr
+-__aeabi_memcpy8:
+-__aeabi_memcpy4:
+-__aeabi_memcpy:
+-3:	cmp   r2, #0
+-	beq   2f
+-	adds  r2, r1, r2
+-1:	ldrb  r3, [r1]
+-	adds  r1, r1, #1
+-	strb  r3, [r0]
+-	adds  r0, r0, #1
+-	cmp   r1, r2
+-	bne   1b
+-2:	bx    lr
+--- a/src/string/arm/__aeabi_memset.s	2024-03-01 10:07:33.000000000 +0800
++++ /dev/null	2024-10-19 22:20:54.374000100 +0800
+@@ -1,31 +0,0 @@
+-.syntax unified
+-
+-.global __aeabi_memclr8
+-.global __aeabi_memclr4
+-.global __aeabi_memclr
+-.global __aeabi_memset8
+-.global __aeabi_memset4
+-.global __aeabi_memset
+-
+-.type __aeabi_memclr8,%function
+-.type __aeabi_memclr4,%function
+-.type __aeabi_memclr,%function
+-.type __aeabi_memset8,%function
+-.type __aeabi_memset4,%function
+-.type __aeabi_memset,%function
+-
+-__aeabi_memclr8:
+-__aeabi_memclr4:
+-__aeabi_memclr:
+-	movs  r2, #0
+-__aeabi_memset8:
+-__aeabi_memset4:
+-__aeabi_memset:
+-	cmp   r1, #0
+-	beq   2f
+-	adds  r1, r0, r1
+-1:	strb  r2, [r0]
+-	adds  r0, r0, #1
+-	cmp   r1, r0
+-	bne   1b
+-2:	bx    lr
+--- a/src/string/arm/memcpy.S	2024-03-01 10:07:33.000000000 +0800
++++ /dev/null	2024-10-19 22:20:54.374000100 +0800
+@@ -1,479 +0,0 @@
+-/*
+- * Copyright (C) 2008 The Android Open Source Project
+- * All rights reserved.
+- *
+- * Redistribution and use in source and binary forms, with or without
+- * modification, are permitted provided that the following conditions
+- * are met:
+- *  * Redistributions of source code must retain the above copyright
+- *    notice, this list of conditions and the following disclaimer.
+- *  * Redistributions in binary form must reproduce the above copyright
+- *    notice, this list of conditions and the following disclaimer in
+- *    the documentation and/or other materials provided with the
+- *    distribution.
+- *
+- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+- * FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+- * COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+- * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+- * BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
+- * OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED
+- * AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+- * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
+- * OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+- * SUCH DAMAGE.
+- */
+-
+-
+-/*
+- * Optimized memcpy() for ARM.
+- *
+- * note that memcpy() always returns the destination pointer,
+- * so we have to preserve R0.
+-  */
+-
+-/*
+- * This file has been modified from the original for use in musl libc.
+- * The main changes are: addition of .type memcpy,%function to make the
+- * code safely callable from thumb mode, adjusting the return
+- * instructions to be compatible with pre-thumb ARM cpus, removal of
+- * prefetch code that is not compatible with older cpus and support for
+- * building as thumb 2 and big-endian.
+- */
+-
+-.syntax unified
+-
+-.global memcpy
+-.type memcpy,%function
+-memcpy:
+-	/* The stack must always be 64-bits aligned to be compliant with the
+-	 * ARM ABI. Since we have to save R0, we might as well save R4
+-	 * which we can use for better pipelining of the reads below
+-	 */
+-	.fnstart
+-	.save       {r0, r4, lr}
+-	stmfd       sp!, {r0, r4, lr}
+-	/* Making room for r5-r11 which will be spilled later */
+-	.pad        #28
+-	sub         sp, sp, #28
+-
+-	/* it simplifies things to take care of len<4 early */
+-	cmp     r2, #4
+-	blo     copy_last_3_and_return
+-
+-	/* compute the offset to align the source
+-	 * offset = (4-(src&3))&3 = -src & 3
+-	 */
+-	rsb     r3, r1, #0
+-	ands    r3, r3, #3
+-	beq     src_aligned
+-
+-	/* align source to 32 bits. We need to insert 2 instructions between
+-	 * a ldr[b|h] and str[b|h] because byte and half-word instructions
+-	 * stall 2 cycles.
+-	 */
+-	movs    r12, r3, lsl #31
+-	sub     r2, r2, r3              /* we know that r3 <= r2 because r2 >= 4 */
+-	ldrbmi r3, [r1], #1
+-	ldrbcs r4, [r1], #1
+-	ldrbcs r12,[r1], #1
+-	strbmi r3, [r0], #1
+-	strbcs r4, [r0], #1
+-	strbcs r12,[r0], #1
+-
+-src_aligned:
+-
+-	/* see if src and dst are aligned together (congruent) */
+-	eor     r12, r0, r1
+-	tst     r12, #3
+-	bne     non_congruent
+-
+-	/* Use post-incriment mode for stm to spill r5-r11 to reserved stack
+-	 * frame. Don't update sp.
+-	 */
+-	stmea   sp, {r5-r11}
+-
+-	/* align the destination to a cache-line */
+-	rsb     r3, r0, #0
+-	ands    r3, r3, #0x1C
+-	beq     congruent_aligned32
+-	cmp     r3, r2
+-	andhi   r3, r2, #0x1C
+-
+-	/* conditionnaly copies 0 to 7 words (length in r3) */
+-	movs    r12, r3, lsl #28
+-	ldmcs   r1!, {r4, r5, r6, r7}           /* 16 bytes */
+-	ldmmi   r1!, {r8, r9}                   /*  8 bytes */
+-	stmcs   r0!, {r4, r5, r6, r7}
+-	stmmi   r0!, {r8, r9}
+-	tst     r3, #0x4
+-	ldrne   r10,[r1], #4                    /*  4 bytes */
+-	strne   r10,[r0], #4
+-	sub     r2, r2, r3
+-
+-congruent_aligned32:
+-	/*
+-	 * here source is aligned to 32 bytes.
+-	 */
+-
+-cached_aligned32:
+-	subs    r2, r2, #32
+-	blo     less_than_32_left
+-
+-	/*
+-	 * We preload a cache-line up to 64 bytes ahead. On the 926, this will
+-	 * stall only until the requested world is fetched, but the linefill
+-	 * continues in the the background.
+-	 * While the linefill is going, we write our previous cache-line
+-	 * into the write-buffer (which should have some free space).
+-	 * When the linefill is done, the writebuffer will
+-	 * start dumping its content into memory
+-	 *
+-	 * While all this is going, we then load a full cache line into
+-	 * 8 registers, this cache line should be in the cache by now
+-	 * (or partly in the cache).
+-	 *
+-	 * This code should work well regardless of the source/dest alignment.
+-	 *
+-	 */
+-
+-	/* Align the preload register to a cache-line because the cpu does
+-	 * "critical word first" (the first word requested is loaded first).
+-	 */
+-	@ bic           r12, r1, #0x1F
+-	@ add           r12, r12, #64
+-
+-1:      ldmia   r1!, { r4-r11 }
+-	subs    r2, r2, #32
+-
+-	/* 
+-	 * NOTE: if r12 is more than 64 ahead of r1, the following ldrhi
+-	 * for ARM9 preload will not be safely guarded by the preceding subs.
+-	 * When it is safely guarded the only possibility to have SIGSEGV here
+-	 * is because the caller overstates the length.
+-	 */
+-	@ ldrhi         r3, [r12], #32      /* cheap ARM9 preload */
+-	stmia   r0!, { r4-r11 }
+-	bhs     1b
+-
+-	add     r2, r2, #32
+-
+-less_than_32_left:
+-	/*
+-	 * less than 32 bytes left at this point (length in r2)
+-	 */
+-
+-	/* skip all this if there is nothing to do, which should
+-	 * be a common case (if not executed the code below takes
+-	 * about 16 cycles)
+-	 */
+-	tst     r2, #0x1F
+-	beq     1f
+-
+-	/* conditionnaly copies 0 to 31 bytes */
+-	movs    r12, r2, lsl #28
+-	ldmcs   r1!, {r4, r5, r6, r7}           /* 16 bytes */
+-	ldmmi   r1!, {r8, r9}                   /*  8 bytes */
+-	stmcs   r0!, {r4, r5, r6, r7}
+-	stmmi   r0!, {r8, r9}
+-	movs    r12, r2, lsl #30
+-	ldrcs   r3, [r1], #4                    /*  4 bytes */
+-	ldrhmi r4, [r1], #2                     /*  2 bytes */
+-	strcs   r3, [r0], #4
+-	strhmi r4, [r0], #2
+-	tst     r2, #0x1
+-	ldrbne r3, [r1]                         /*  last byte  */
+-	strbne r3, [r0]
+-
+-	/* we're done! restore everything and return */
+-1:      ldmfd   sp!, {r5-r11}
+-	ldmfd   sp!, {r0, r4, lr}
+-	bx      lr
+-
+-	/********************************************************************/
+-
+-non_congruent:
+-	/*
+-	 * here source is aligned to 4 bytes
+-	 * but destination is not.
+-	 *
+-	 * in the code below r2 is the number of bytes read
+-	 * (the number of bytes written is always smaller, because we have
+-	 * partial words in the shift queue)
+-	 */
+-	cmp     r2, #4
+-	blo     copy_last_3_and_return
+-
+-	/* Use post-incriment mode for stm to spill r5-r11 to reserved stack
+-	 * frame. Don't update sp.
+-	 */
+-	stmea   sp, {r5-r11}
+-
+-	/* compute shifts needed to align src to dest */
+-	rsb     r5, r0, #0
+-	and     r5, r5, #3                      /* r5 = # bytes in partial words */
+-	mov     r12, r5, lsl #3         /* r12 = right */
+-	rsb     lr, r12, #32            /* lr = left  */
+-
+-	/* read the first word */
+-	ldr     r3, [r1], #4
+-	sub     r2, r2, #4
+-
+-	/* write a partial word (0 to 3 bytes), such that destination
+-	 * becomes aligned to 32 bits (r5 = nb of words to copy for alignment)
+-	 */
+-	movs    r5, r5, lsl #31
+-
+-#if __ARMEB__
+-	movmi   r3, r3, ror #24
+-	strbmi	r3, [r0], #1
+-	movcs   r3, r3, ror #24
+-	strbcs	r3, [r0], #1
+-	movcs   r3, r3, ror #24
+-	strbcs	r3, [r0], #1
+-#else
+-	strbmi r3, [r0], #1
+-	movmi   r3, r3, lsr #8
+-	strbcs r3, [r0], #1
+-	movcs   r3, r3, lsr #8
+-	strbcs r3, [r0], #1
+-	movcs   r3, r3, lsr #8
+-#endif
+-
+-	cmp     r2, #4
+-	blo     partial_word_tail
+-
+-#if __ARMEB__
+-	mov	r3, r3, lsr r12
+-	mov	r3, r3, lsl r12
+-#endif
+-
+-	/* Align destination to 32 bytes (cache line boundary) */
+-1:      tst     r0, #0x1c
+-	beq     2f
+-	ldr     r5, [r1], #4
+-	sub     r2, r2, #4
+-#if __ARMEB__
+-	mov     r4, r5,                 lsr lr
+-	orr     r4, r4, r3
+-	mov     r3, r5,                 lsl r12
+-#else
+-	mov     r4, r5,                 lsl lr
+-	orr     r4, r4, r3
+-	mov     r3, r5,                 lsr r12
+-#endif
+-	str     r4, [r0], #4
+-	cmp     r2, #4
+-	bhs     1b
+-	blo     partial_word_tail
+-
+-	/* copy 32 bytes at a time */
+-2:      subs    r2, r2, #32
+-	blo     less_than_thirtytwo
+-
+-	/* Use immediate mode for the shifts, because there is an extra cycle
+-	 * for register shifts, which could account for up to 50% of
+-	 * performance hit.
+-	 */
+-
+-	cmp     r12, #24
+-	beq     loop24
+-	cmp     r12, #8
+-	beq     loop8
+-
+-loop16:
+-	ldr     r12, [r1], #4
+-1:      mov     r4, r12
+-	ldmia   r1!, {   r5,r6,r7,  r8,r9,r10,r11}
+-	subs    r2, r2, #32
+-	ldrhs   r12, [r1], #4
+-#if __ARMEB__
+-	orr     r3, r3, r4, lsr #16
+-	mov     r4, r4, lsl #16
+-	orr     r4, r4, r5, lsr #16
+-	mov     r5, r5, lsl #16
+-	orr     r5, r5, r6, lsr #16
+-	mov     r6, r6, lsl #16
+-	orr     r6, r6, r7, lsr #16
+-	mov     r7, r7, lsl #16
+-	orr     r7, r7, r8, lsr #16
+-	mov     r8, r8, lsl #16
+-	orr     r8, r8, r9, lsr #16
+-	mov     r9, r9, lsl #16
+-	orr     r9, r9, r10, lsr #16
+-	mov     r10, r10,               lsl #16
+-	orr     r10, r10, r11, lsr #16
+-	stmia   r0!, {r3,r4,r5,r6, r7,r8,r9,r10}
+-	mov     r3, r11, lsl #16
+-#else
+-	orr     r3, r3, r4, lsl #16
+-	mov     r4, r4, lsr #16
+-	orr     r4, r4, r5, lsl #16
+-	mov     r5, r5, lsr #16
+-	orr     r5, r5, r6, lsl #16
+-	mov     r6, r6, lsr #16
+-	orr     r6, r6, r7, lsl #16
+-	mov     r7, r7, lsr #16
+-	orr     r7, r7, r8, lsl #16
+-	mov     r8, r8, lsr #16
+-	orr     r8, r8, r9, lsl #16
+-	mov     r9, r9, lsr #16
+-	orr     r9, r9, r10, lsl #16
+-	mov     r10, r10,               lsr #16
+-	orr     r10, r10, r11, lsl #16
+-	stmia   r0!, {r3,r4,r5,r6, r7,r8,r9,r10}
+-	mov     r3, r11, lsr #16
+-#endif
+-	bhs     1b
+-	b       less_than_thirtytwo
+-
+-loop8:
+-	ldr     r12, [r1], #4
+-1:      mov     r4, r12
+-	ldmia   r1!, {   r5,r6,r7,  r8,r9,r10,r11}
+-	subs    r2, r2, #32
+-	ldrhs   r12, [r1], #4
+-#if __ARMEB__
+-	orr     r3, r3, r4, lsr #24
+-	mov     r4, r4, lsl #8
+-	orr     r4, r4, r5, lsr #24
+-	mov     r5, r5, lsl #8
+-	orr     r5, r5, r6, lsr #24
+-	mov     r6, r6,  lsl #8
+-	orr     r6, r6, r7, lsr #24
+-	mov     r7, r7,  lsl #8
+-	orr     r7, r7, r8,             lsr #24
+-	mov     r8, r8,  lsl #8
+-	orr     r8, r8, r9,             lsr #24
+-	mov     r9, r9,  lsl #8
+-	orr     r9, r9, r10,    lsr #24
+-	mov     r10, r10, lsl #8
+-	orr     r10, r10, r11,  lsr #24
+-	stmia   r0!, {r3,r4,r5,r6, r7,r8,r9,r10}
+-	mov     r3, r11, lsl #8
+-#else
+-	orr     r3, r3, r4, lsl #24
+-	mov     r4, r4, lsr #8
+-	orr     r4, r4, r5, lsl #24
+-	mov     r5, r5, lsr #8
+-	orr     r5, r5, r6, lsl #24
+-	mov     r6, r6,  lsr #8
+-	orr     r6, r6, r7, lsl #24
+-	mov     r7, r7,  lsr #8
+-	orr     r7, r7, r8,             lsl #24
+-	mov     r8, r8,  lsr #8
+-	orr     r8, r8, r9,             lsl #24
+-	mov     r9, r9,  lsr #8
+-	orr     r9, r9, r10,    lsl #24
+-	mov     r10, r10, lsr #8
+-	orr     r10, r10, r11,  lsl #24
+-	stmia   r0!, {r3,r4,r5,r6, r7,r8,r9,r10}
+-	mov     r3, r11, lsr #8
+-#endif
+-	bhs     1b
+-	b       less_than_thirtytwo
+-
+-loop24:
+-	ldr     r12, [r1], #4
+-1:      mov     r4, r12
+-	ldmia   r1!, {   r5,r6,r7,  r8,r9,r10,r11}
+-	subs    r2, r2, #32
+-	ldrhs   r12, [r1], #4
+-#if __ARMEB__
+-	orr     r3, r3, r4, lsr #8
+-	mov     r4, r4, lsl #24
+-	orr     r4, r4, r5, lsr #8
+-	mov     r5, r5, lsl #24
+-	orr     r5, r5, r6, lsr #8
+-	mov     r6, r6, lsl #24
+-	orr     r6, r6, r7, lsr #8
+-	mov     r7, r7, lsl #24
+-	orr     r7, r7, r8, lsr #8
+-	mov     r8, r8, lsl #24
+-	orr     r8, r8, r9, lsr #8
+-	mov     r9, r9, lsl #24
+-	orr     r9, r9, r10, lsr #8
+-	mov     r10, r10, lsl #24
+-	orr     r10, r10, r11, lsr #8
+-	stmia   r0!, {r3,r4,r5,r6, r7,r8,r9,r10}
+-	mov     r3, r11, lsl #24
+-#else
+-	orr     r3, r3, r4, lsl #8
+-	mov     r4, r4, lsr #24
+-	orr     r4, r4, r5, lsl #8
+-	mov     r5, r5, lsr #24
+-	orr     r5, r5, r6, lsl #8
+-	mov     r6, r6, lsr #24
+-	orr     r6, r6, r7, lsl #8
+-	mov     r7, r7, lsr #24
+-	orr     r7, r7, r8, lsl #8
+-	mov     r8, r8, lsr #24
+-	orr     r8, r8, r9, lsl #8
+-	mov     r9, r9, lsr #24
+-	orr     r9, r9, r10, lsl #8
+-	mov     r10, r10, lsr #24
+-	orr     r10, r10, r11, lsl #8
+-	stmia   r0!, {r3,r4,r5,r6, r7,r8,r9,r10}
+-	mov     r3, r11, lsr #24
+-#endif
+-	bhs     1b
+-
+-less_than_thirtytwo:
+-	/* copy the last 0 to 31 bytes of the source */
+-	rsb     r12, lr, #32            /* we corrupted r12, recompute it  */
+-	add     r2, r2, #32
+-	cmp     r2, #4
+-	blo     partial_word_tail
+-
+-1:      ldr     r5, [r1], #4
+-	sub     r2, r2, #4
+-#if __ARMEB__
+-	mov     r4, r5,                 lsr lr
+-	orr     r4, r4, r3
+-	mov     r3,     r5,                     lsl r12
+-#else
+-	mov     r4, r5,                 lsl lr
+-	orr     r4, r4, r3
+-	mov     r3,     r5,                     lsr r12
+-#endif
+-	str     r4, [r0], #4
+-	cmp     r2, #4
+-	bhs     1b
+-
+-partial_word_tail:
+-	/* we have a partial word in the input buffer */
+-	movs    r5, lr, lsl #(31-3)
+-#if __ARMEB__
+-	movmi   r3, r3, ror #24
+-	strbmi r3, [r0], #1
+-	movcs   r3, r3, ror #24
+-	strbcs r3, [r0], #1
+-	movcs   r3, r3, ror #24
+-	strbcs r3, [r0], #1
+-#else
+-	strbmi r3, [r0], #1
+-	movmi   r3, r3, lsr #8
+-	strbcs r3, [r0], #1
+-	movcs   r3, r3, lsr #8
+-	strbcs r3, [r0], #1
+-#endif
+-
+-	/* Refill spilled registers from the stack. Don't update sp. */
+-	ldmfd   sp, {r5-r11}
+-
+-copy_last_3_and_return:
+-	movs    r2, r2, lsl #31 /* copy remaining 0, 1, 2 or 3 bytes */
+-	ldrbmi r2, [r1], #1
+-	ldrbcs r3, [r1], #1
+-	ldrbcs r12,[r1]
+-	strbmi r2, [r0], #1
+-	strbcs r3, [r0], #1
+-	strbcs r12,[r0]
+-
+-	/* we're done! restore sp and spilled registers and return */
+-	add     sp,  sp, #28
+-	ldmfd   sp!, {r0, r4, lr}
+-	bx      lr
+-
+--- a/src/string/memcpy.c	2024-03-01 10:07:33.000000000 +0800
++++ /dev/null	2024-10-19 22:20:54.374000100 +0800
+@@ -1,124 +0,0 @@
+-#include <string.h>
+-#include <stdint.h>
+-#include <endian.h>
+-
+-void *memcpy(void *restrict dest, const void *restrict src, size_t n)
+-{
+-	unsigned char *d = dest;
+-	const unsigned char *s = src;
+-
+-#ifdef __GNUC__
+-
+-#if __BYTE_ORDER == __LITTLE_ENDIAN
+-#define LS >>
+-#define RS <<
+-#else
+-#define LS <<
+-#define RS >>
+-#endif
+-
+-	typedef uint32_t __attribute__((__may_alias__)) u32;
+-	uint32_t w, x;
+-
+-	for (; (uintptr_t)s % 4 && n; n--) *d++ = *s++;
+-
+-	if ((uintptr_t)d % 4 == 0) {
+-		for (; n>=16; s+=16, d+=16, n-=16) {
+-			*(u32 *)(d+0) = *(u32 *)(s+0);
+-			*(u32 *)(d+4) = *(u32 *)(s+4);
+-			*(u32 *)(d+8) = *(u32 *)(s+8);
+-			*(u32 *)(d+12) = *(u32 *)(s+12);
+-		}
+-		if (n&8) {
+-			*(u32 *)(d+0) = *(u32 *)(s+0);
+-			*(u32 *)(d+4) = *(u32 *)(s+4);
+-			d += 8; s += 8;
+-		}
+-		if (n&4) {
+-			*(u32 *)(d+0) = *(u32 *)(s+0);
+-			d += 4; s += 4;
+-		}
+-		if (n&2) {
+-			*d++ = *s++; *d++ = *s++;
+-		}
+-		if (n&1) {
+-			*d = *s;
+-		}
+-		return dest;
+-	}
+-
+-	if (n >= 32) switch ((uintptr_t)d % 4) {
+-	case 1:
+-		w = *(u32 *)s;
+-		*d++ = *s++;
+-		*d++ = *s++;
+-		*d++ = *s++;
+-		n -= 3;
+-		for (; n>=17; s+=16, d+=16, n-=16) {
+-			x = *(u32 *)(s+1);
+-			*(u32 *)(d+0) = (w LS 24) | (x RS 8);
+-			w = *(u32 *)(s+5);
+-			*(u32 *)(d+4) = (x LS 24) | (w RS 8);
+-			x = *(u32 *)(s+9);
+-			*(u32 *)(d+8) = (w LS 24) | (x RS 8);
+-			w = *(u32 *)(s+13);
+-			*(u32 *)(d+12) = (x LS 24) | (w RS 8);
+-		}
+-		break;
+-	case 2:
+-		w = *(u32 *)s;
+-		*d++ = *s++;
+-		*d++ = *s++;
+-		n -= 2;
+-		for (; n>=18; s+=16, d+=16, n-=16) {
+-			x = *(u32 *)(s+2);
+-			*(u32 *)(d+0) = (w LS 16) | (x RS 16);
+-			w = *(u32 *)(s+6);
+-			*(u32 *)(d+4) = (x LS 16) | (w RS 16);
+-			x = *(u32 *)(s+10);
+-			*(u32 *)(d+8) = (w LS 16) | (x RS 16);
+-			w = *(u32 *)(s+14);
+-			*(u32 *)(d+12) = (x LS 16) | (w RS 16);
+-		}
+-		break;
+-	case 3:
+-		w = *(u32 *)s;
+-		*d++ = *s++;
+-		n -= 1;
+-		for (; n>=19; s+=16, d+=16, n-=16) {
+-			x = *(u32 *)(s+3);
+-			*(u32 *)(d+0) = (w LS 8) | (x RS 24);
+-			w = *(u32 *)(s+7);
+-			*(u32 *)(d+4) = (x LS 8) | (w RS 24);
+-			x = *(u32 *)(s+11);
+-			*(u32 *)(d+8) = (w LS 8) | (x RS 24);
+-			w = *(u32 *)(s+15);
+-			*(u32 *)(d+12) = (x LS 8) | (w RS 24);
+-		}
+-		break;
+-	}
+-	if (n&16) {
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-	}
+-	if (n&8) {
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-	}
+-	if (n&4) {
+-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
+-	}
+-	if (n&2) {
+-		*d++ = *s++; *d++ = *s++;
+-	}
+-	if (n&1) {
+-		*d = *s;
+-	}
+-	return dest;
+-#endif
+-
+-	for (; n; n--) *d++ = *s++;
+-	return dest;
+-}
+--- a/src/string/memset.c	2024-03-01 10:07:33.000000000 +0800
++++ /dev/null	2024-10-19 22:20:54.374000100 +0800
+@@ -1,90 +0,0 @@
+-#include <string.h>
+-#include <stdint.h>
+-
+-void *memset(void *dest, int c, size_t n)
+-{
+-	unsigned char *s = dest;
+-	size_t k;
+-
+-	/* Fill head and tail with minimal branching. Each
+-	 * conditional ensures that all the subsequently used
+-	 * offsets are well-defined and in the dest region. */
+-
+-	if (!n) return dest;
+-	s[0] = c;
+-	s[n-1] = c;
+-	if (n <= 2) return dest;
+-	s[1] = c;
+-	s[2] = c;
+-	s[n-2] = c;
+-	s[n-3] = c;
+-	if (n <= 6) return dest;
+-	s[3] = c;
+-	s[n-4] = c;
+-	if (n <= 8) return dest;
+-
+-	/* Advance pointer to align it at a 4-byte boundary,
+-	 * and truncate n to a multiple of 4. The previous code
+-	 * already took care of any head/tail that get cut off
+-	 * by the alignment. */
+-
+-	k = -(uintptr_t)s & 3;
+-	s += k;
+-	n -= k;
+-	n &= -4;
+-
+-#ifdef __GNUC__
+-	typedef uint32_t __attribute__((__may_alias__)) u32;
+-	typedef uint64_t __attribute__((__may_alias__)) u64;
+-
+-	u32 c32 = ((u32)-1)/255 * (unsigned char)c;
+-
+-	/* In preparation to copy 32 bytes at a time, aligned on
+-	 * an 8-byte bounary, fill head/tail up to 28 bytes each.
+-	 * As in the initial byte-based head/tail fill, each
+-	 * conditional below ensures that the subsequent offsets
+-	 * are valid (e.g. !(n<=24) implies n>=28). */
+-
+-	*(u32 *)(s+0) = c32;
+-	*(u32 *)(s+n-4) = c32;
+-	if (n <= 8) return dest;
+-	*(u32 *)(s+4) = c32;
+-	*(u32 *)(s+8) = c32;
+-	*(u32 *)(s+n-12) = c32;
+-	*(u32 *)(s+n-8) = c32;
+-	if (n <= 24) return dest;
+-	*(u32 *)(s+12) = c32;
+-	*(u32 *)(s+16) = c32;
+-	*(u32 *)(s+20) = c32;
+-	*(u32 *)(s+24) = c32;
+-	*(u32 *)(s+n-28) = c32;
+-	*(u32 *)(s+n-24) = c32;
+-	*(u32 *)(s+n-20) = c32;
+-	*(u32 *)(s+n-16) = c32;
+-
+-	/* Align to a multiple of 8 so we can fill 64 bits at a time,
+-	 * and avoid writing the same bytes twice as much as is
+-	 * practical without introducing additional branching. */
+-
+-	k = 24 + ((uintptr_t)s & 4);
+-	s += k;
+-	n -= k;
+-
+-	/* If this loop is reached, 28 tail bytes have already been
+-	 * filled, so any remainder when n drops below 32 can be
+-	 * safely ignored. */
+-
+-	u64 c64 = c32 | ((u64)c32 << 32);
+-	for (; n >= 32; n-=32, s+=32) {
+-		*(u64 *)(s+0) = c64;
+-		*(u64 *)(s+8) = c64;
+-		*(u64 *)(s+16) = c64;
+-		*(u64 *)(s+24) = c64;
+-	}
+-#else
+-	/* Pure C fallback with no aliasing violations. */
+-	for (; n; n--, s++) *s = c;
+-#endif
+-
+-	return dest;
+-}
+--- /dev/null	2024-10-19 22:20:54.374000100 +0800
++++ b/src/string/arm/memcpy.S	2024-10-25 00:17:51.492485797 +0800
+@@ -0,0 +1,588 @@
++/*
++ * memcpy - copy memory area
++ *
++ * Copyright (c) 2013-2022, Arm Limited.
++ * SPDX-License-Identifier: MIT OR Apache-2.0 WITH LLVM-exception
++ */
++
++/*
++   This memcpy routine is optimised for Cortex-A15 cores and takes advantage
++   of VFP or NEON when built with the appropriate flags.
++
++   Assumptions:
++
++    ARMv6 (ARMv7-a if using Neon)
++    ARM state
++    Unaligned accesses
++
++ */
++
++#include "asmdefs.h"
++
++	.syntax unified
++	/* This implementation requires ARM state.  */
++	.arm
++
++
++
++	.fpu	neon
++	.arch	armv7-a
++# define FRAME_SIZE	4
++# define USE_VFP
++# define USE_NEON
++
++
++
++
++
++
++
++
++
++
++
++
++
++
++/* Old versions of GAS incorrectly implement the NEON align semantics.  */
++#ifdef BROKEN_ASM_NEON_ALIGN
++#define ALIGN(addr, align) addr,:align
++#else
++#define ALIGN(addr, align) addr:align
++#endif
++
++#define PC_OFFSET	8	/* PC pipeline compensation.  */
++#define INSN_SIZE	4
++
++/* Call parameters.  */
++#define dstin	r0
++#define src	r1
++#define count	r2
++
++/* Locals.  */
++#define tmp1	r3
++#define dst	ip
++#define tmp2	r10
++
++#ifndef USE_NEON
++/* For bulk copies using GP registers.  */
++#define	A_l	r2		/* Call-clobbered.  */
++#define	A_h	r3		/* Call-clobbered.  */
++#define	B_l	r4
++#define	B_h	r5
++#define	C_l	r6
++#define	C_h	r7
++#define	D_l	r8
++#define	D_h	r9
++#endif
++
++/* Number of lines ahead to pre-fetch data.  If you change this the code
++   below will need adjustment to compensate.  */
++
++#define prefetch_lines	5
++
++#ifdef USE_VFP
++	.macro	cpy_line_vfp vreg, base
++	vstr	\vreg, [dst, #\base]
++	vldr	\vreg, [src, #\base]
++	vstr	d0, [dst, #\base + 8]
++	vldr	d0, [src, #\base + 8]
++	vstr	d1, [dst, #\base + 16]
++	vldr	d1, [src, #\base + 16]
++	vstr	d2, [dst, #\base + 24]
++	vldr	d2, [src, #\base + 24]
++	vstr	\vreg, [dst, #\base + 32]
++	vldr	\vreg, [src, #\base + prefetch_lines * 64 - 32]
++	vstr	d0, [dst, #\base + 40]
++	vldr	d0, [src, #\base + 40]
++	vstr	d1, [dst, #\base + 48]
++	vldr	d1, [src, #\base + 48]
++	vstr	d2, [dst, #\base + 56]
++	vldr	d2, [src, #\base + 56]
++	.endm
++
++	.macro	cpy_tail_vfp vreg, base
++	vstr	\vreg, [dst, #\base]
++	vldr	\vreg, [src, #\base]
++	vstr	d0, [dst, #\base + 8]
++	vldr	d0, [src, #\base + 8]
++	vstr	d1, [dst, #\base + 16]
++	vldr	d1, [src, #\base + 16]
++	vstr	d2, [dst, #\base + 24]
++	vldr	d2, [src, #\base + 24]
++	vstr	\vreg, [dst, #\base + 32]
++	vstr	d0, [dst, #\base + 40]
++	vldr	d0, [src, #\base + 40]
++	vstr	d1, [dst, #\base + 48]
++	vldr	d1, [src, #\base + 48]
++	vstr	d2, [dst, #\base + 56]
++	vldr	d2, [src, #\base + 56]
++	.endm
++#endif
++
++.global memcpy
++.type memcpy,%function
++memcpy:
++
++	mov	dst, dstin	/* Preserve dstin, we need to return it.  */
++	cmp	count, #64
++	bhs	L(cpy_not_short)
++	/* Deal with small copies quickly by dropping straight into the
++	   exit block.  */
++
++L(tail63unaligned):
++#ifdef USE_NEON
++	and	tmp1, count, #0x38
++	rsb	tmp1, tmp1, #(56 - PC_OFFSET + INSN_SIZE)
++	add	pc, pc, tmp1
++	vld1.8	{d0}, [src]!	/* 14 words to go.  */
++	vst1.8	{d0}, [dst]!
++	vld1.8	{d0}, [src]!	/* 12 words to go.  */
++	vst1.8	{d0}, [dst]!
++	vld1.8	{d0}, [src]!	/* 10 words to go.  */
++	vst1.8	{d0}, [dst]!
++	vld1.8	{d0}, [src]!	/* 8 words to go.  */
++	vst1.8	{d0}, [dst]!
++	vld1.8	{d0}, [src]!	/* 6 words to go.  */
++	vst1.8	{d0}, [dst]!
++	vld1.8	{d0}, [src]!	/* 4 words to go.  */
++	vst1.8	{d0}, [dst]!
++	vld1.8	{d0}, [src]!	/* 2 words to go.  */
++	vst1.8	{d0}, [dst]!
++
++	tst	count, #4
++	ldrne	tmp1, [src], #4
++	strne	tmp1, [dst], #4
++#else
++	/* Copy up to 15 full words of data.  May not be aligned.  */
++	/* Cannot use VFP for unaligned data.  */
++	and	tmp1, count, #0x3c
++	add	dst, dst, tmp1
++	add	src, src, tmp1
++	rsb	tmp1, tmp1, #(60 - PC_OFFSET/2 + INSN_SIZE/2)
++	/* Jump directly into the sequence below at the correct offset.  */
++	add	pc, pc, tmp1, lsl #1
++
++	ldr	tmp1, [src, #-60]	/* 15 words to go.  */
++	str	tmp1, [dst, #-60]
++
++	ldr	tmp1, [src, #-56]	/* 14 words to go.  */
++	str	tmp1, [dst, #-56]
++	ldr	tmp1, [src, #-52]
++	str	tmp1, [dst, #-52]
++
++	ldr	tmp1, [src, #-48]	/* 12 words to go.  */
++	str	tmp1, [dst, #-48]
++	ldr	tmp1, [src, #-44]
++	str	tmp1, [dst, #-44]
++
++	ldr	tmp1, [src, #-40]	/* 10 words to go.  */
++	str	tmp1, [dst, #-40]
++	ldr	tmp1, [src, #-36]
++	str	tmp1, [dst, #-36]
++
++	ldr	tmp1, [src, #-32]	/* 8 words to go.  */
++	str	tmp1, [dst, #-32]
++	ldr	tmp1, [src, #-28]
++	str	tmp1, [dst, #-28]
++
++	ldr	tmp1, [src, #-24]	/* 6 words to go.  */
++	str	tmp1, [dst, #-24]
++	ldr	tmp1, [src, #-20]
++	str	tmp1, [dst, #-20]
++
++	ldr	tmp1, [src, #-16]	/* 4 words to go.  */
++	str	tmp1, [dst, #-16]
++	ldr	tmp1, [src, #-12]
++	str	tmp1, [dst, #-12]
++
++	ldr	tmp1, [src, #-8]	/* 2 words to go.  */
++	str	tmp1, [dst, #-8]
++	ldr	tmp1, [src, #-4]
++	str	tmp1, [dst, #-4]
++#endif
++
++	lsls	count, count, #31
++	ldrhcs	tmp1, [src], #2
++	ldrbne	src, [src]		/* Src is dead, use as a scratch.  */
++	strhcs	tmp1, [dst], #2
++	strbne	src, [dst]
++	bx	lr
++
++L(cpy_not_short):
++	/* At least 64 bytes to copy, but don't know the alignment yet.  */
++	str	tmp2, [sp, #-FRAME_SIZE]!
++	and	tmp2, src, #7
++	and	tmp1, dst, #7
++	cmp	tmp1, tmp2
++	bne	L(cpy_notaligned)
++
++#ifdef USE_VFP
++	/* Magic dust alert!  Force VFP on Cortex-A9.  Experiments show
++	   that the FP pipeline is much better at streaming loads and
++	   stores.  This is outside the critical loop.  */
++	vmov.f32	s0, s0
++#endif
++
++	/* SRC and DST have the same mutual 64-bit alignment, but we may
++	   still need to pre-copy some bytes to get to natural alignment.
++	   We bring SRC and DST into full 64-bit alignment.  */
++	lsls	tmp2, dst, #29
++	beq	1f
++	rsbs	tmp2, tmp2, #0
++	sub	count, count, tmp2, lsr #29
++	ldrmi	tmp1, [src], #4
++	strmi	tmp1, [dst], #4
++	lsls	tmp2, tmp2, #2
++	ldrhcs	tmp1, [src], #2
++	ldrbne	tmp2, [src], #1
++	strhcs	tmp1, [dst], #2
++	strbne	tmp2, [dst], #1
++
++1:
++	subs	tmp2, count, #64	/* Use tmp2 for count.  */
++	blo	L(tail63aligned)
++
++	cmp	tmp2, #512
++	bhs	L(cpy_body_long)
++
++L(cpy_body_medium):			/* Count in tmp2.  */
++#ifdef USE_VFP
++1:
++	vldr	d0, [src, #0]
++	subs	tmp2, tmp2, #64
++	vldr	d1, [src, #8]
++	vstr	d0, [dst, #0]
++	vldr	d0, [src, #16]
++	vstr	d1, [dst, #8]
++	vldr	d1, [src, #24]
++	vstr	d0, [dst, #16]
++	vldr	d0, [src, #32]
++	vstr	d1, [dst, #24]
++	vldr	d1, [src, #40]
++	vstr	d0, [dst, #32]
++	vldr	d0, [src, #48]
++	vstr	d1, [dst, #40]
++	vldr	d1, [src, #56]
++	vstr	d0, [dst, #48]
++	add	src, src, #64
++	vstr	d1, [dst, #56]
++	add	dst, dst, #64
++	bhs	1b
++	tst	tmp2, #0x3f
++	beq	L(done)
++
++L(tail63aligned):			/* Count in tmp2.  */
++	and	tmp1, tmp2, #0x38
++	add	dst, dst, tmp1
++	add	src, src, tmp1
++	rsb	tmp1, tmp1, #(56 - PC_OFFSET + INSN_SIZE)
++	add	pc, pc, tmp1
++
++	vldr	d0, [src, #-56]	/* 14 words to go.  */
++	vstr	d0, [dst, #-56]
++	vldr	d0, [src, #-48]	/* 12 words to go.  */
++	vstr	d0, [dst, #-48]
++	vldr	d0, [src, #-40]	/* 10 words to go.  */
++	vstr	d0, [dst, #-40]
++	vldr	d0, [src, #-32]	/* 8 words to go.  */
++	vstr	d0, [dst, #-32]
++	vldr	d0, [src, #-24]	/* 6 words to go.  */
++	vstr	d0, [dst, #-24]
++	vldr	d0, [src, #-16]	/* 4 words to go.  */
++	vstr	d0, [dst, #-16]
++	vldr	d0, [src, #-8]	/* 2 words to go.  */
++	vstr	d0, [dst, #-8]
++#else
++	sub	src, src, #8
++	sub	dst, dst, #8
++1:
++	ldrd	A_l, A_h, [src, #8]
++	strd	A_l, A_h, [dst, #8]
++	ldrd	A_l, A_h, [src, #16]
++	strd	A_l, A_h, [dst, #16]
++	ldrd	A_l, A_h, [src, #24]
++	strd	A_l, A_h, [dst, #24]
++	ldrd	A_l, A_h, [src, #32]
++	strd	A_l, A_h, [dst, #32]
++	ldrd	A_l, A_h, [src, #40]
++	strd	A_l, A_h, [dst, #40]
++	ldrd	A_l, A_h, [src, #48]
++	strd	A_l, A_h, [dst, #48]
++	ldrd	A_l, A_h, [src, #56]
++	strd	A_l, A_h, [dst, #56]
++	ldrd	A_l, A_h, [src, #64]!
++	strd	A_l, A_h, [dst, #64]!
++	subs	tmp2, tmp2, #64
++	bhs	1b
++	tst	tmp2, #0x3f
++	bne	1f
++	ldr	tmp2,[sp], #FRAME_SIZE
++	bx	lr
++1:
++	add	src, src, #8
++	add	dst, dst, #8
++
++L(tail63aligned):			/* Count in tmp2.  */
++	/* Copy up to 7 d-words of data.  Similar to Ltail63unaligned, but
++	   we know that the src and dest are 64-bit aligned so we can use
++	   LDRD/STRD to improve efficiency.  */
++	/* TMP2 is now negative, but we don't care about that.  The bottom
++	   six bits still tell us how many bytes are left to copy.  */
++
++	and	tmp1, tmp2, #0x38
++	add	dst, dst, tmp1
++	add	src, src, tmp1
++	rsb	tmp1, tmp1, #(56 - PC_OFFSET + INSN_SIZE)
++	add	pc, pc, tmp1
++	ldrd	A_l, A_h, [src, #-56]	/* 14 words to go.  */
++	strd	A_l, A_h, [dst, #-56]
++	ldrd	A_l, A_h, [src, #-48]	/* 12 words to go.  */
++	strd	A_l, A_h, [dst, #-48]
++	ldrd	A_l, A_h, [src, #-40]	/* 10 words to go.  */
++	strd	A_l, A_h, [dst, #-40]
++	ldrd	A_l, A_h, [src, #-32]	/* 8 words to go.  */
++	strd	A_l, A_h, [dst, #-32]
++	ldrd	A_l, A_h, [src, #-24]	/* 6 words to go.  */
++	strd	A_l, A_h, [dst, #-24]
++	ldrd	A_l, A_h, [src, #-16]	/* 4 words to go.  */
++	strd	A_l, A_h, [dst, #-16]
++	ldrd	A_l, A_h, [src, #-8]	/* 2 words to go.  */
++	strd	A_l, A_h, [dst, #-8]
++
++#endif
++	tst	tmp2, #4
++	ldrne	tmp1, [src], #4
++	strne	tmp1, [dst], #4
++	lsls	tmp2, tmp2, #31		/* Count (tmp2) now dead. */
++	ldrhcs	tmp1, [src], #2
++	ldrbne	tmp2, [src]
++	strhcs	tmp1, [dst], #2
++	strbne	tmp2, [dst]
++
++L(done):
++	ldr	tmp2, [sp], #FRAME_SIZE
++	bx	lr
++
++L(cpy_body_long):			/* Count in tmp2.  */
++
++	/* Long copy.  We know that there's at least (prefetch_lines * 64)
++	   bytes to go.  */
++#ifdef USE_VFP
++	/* Don't use PLD.  Instead, read some data in advance of the current
++	   copy position into a register.  This should act like a PLD
++	   operation but we won't have to repeat the transfer.  */
++
++	vldr	d3, [src, #0]
++	vldr	d4, [src, #64]
++	vldr	d5, [src, #128]
++	vldr	d6, [src, #192]
++	vldr	d7, [src, #256]
++
++	vldr	d0, [src, #8]
++	vldr	d1, [src, #16]
++	vldr	d2, [src, #24]
++	add	src, src, #32
++
++	subs	tmp2, tmp2, #prefetch_lines * 64 * 2
++	blo	2f
++1:
++	cpy_line_vfp	d3, 0
++	cpy_line_vfp	d4, 64
++	cpy_line_vfp	d5, 128
++	add	dst, dst, #3 * 64
++	add	src, src, #3 * 64
++	cpy_line_vfp	d6, 0
++	cpy_line_vfp	d7, 64
++	add	dst, dst, #2 * 64
++	add	src, src, #2 * 64
++	subs	tmp2, tmp2, #prefetch_lines * 64
++	bhs	1b
++
++2:
++	cpy_tail_vfp	d3, 0
++	cpy_tail_vfp	d4, 64
++	cpy_tail_vfp	d5, 128
++	add	src, src, #3 * 64
++	add	dst, dst, #3 * 64
++	cpy_tail_vfp	d6, 0
++	vstr	d7, [dst, #64]
++	vldr	d7, [src, #64]
++	vstr	d0, [dst, #64 + 8]
++	vldr	d0, [src, #64 + 8]
++	vstr	d1, [dst, #64 + 16]
++	vldr	d1, [src, #64 + 16]
++	vstr	d2, [dst, #64 + 24]
++	vldr	d2, [src, #64 + 24]
++	vstr	d7, [dst, #64 + 32]
++	add	src, src, #96
++	vstr	d0, [dst, #64 + 40]
++	vstr	d1, [dst, #64 + 48]
++	vstr	d2, [dst, #64 + 56]
++	add	dst, dst, #128
++	add	tmp2, tmp2, #prefetch_lines * 64
++	b	L(cpy_body_medium)
++#else
++	/* Long copy.  Use an SMS style loop to maximize the I/O
++	   bandwidth of the core.  We don't have enough spare registers
++	   to synthesise prefetching, so use PLD operations.  */
++	/* Pre-bias src and dst.  */
++	sub	src, src, #8
++	sub	dst, dst, #8
++	pld	[src, #8]
++	pld	[src, #72]
++	subs	tmp2, tmp2, #64
++	pld	[src, #136]
++	ldrd	A_l, A_h, [src, #8]
++	strd	B_l, B_h, [sp, #8]
++	ldrd	B_l, B_h, [src, #16]
++	strd	C_l, C_h, [sp, #16]
++	ldrd	C_l, C_h, [src, #24]
++	strd	D_l, D_h, [sp, #24]
++	pld	[src, #200]
++	ldrd	D_l, D_h, [src, #32]!
++	b	1f
++	.p2align	6
++2:
++	pld	[src, #232]
++	strd	A_l, A_h, [dst, #40]
++	ldrd	A_l, A_h, [src, #40]
++	strd	B_l, B_h, [dst, #48]
++	ldrd	B_l, B_h, [src, #48]
++	strd	C_l, C_h, [dst, #56]
++	ldrd	C_l, C_h, [src, #56]
++	strd	D_l, D_h, [dst, #64]!
++	ldrd	D_l, D_h, [src, #64]!
++	subs	tmp2, tmp2, #64
++1:
++	strd	A_l, A_h, [dst, #8]
++	ldrd	A_l, A_h, [src, #8]
++	strd	B_l, B_h, [dst, #16]
++	ldrd	B_l, B_h, [src, #16]
++	strd	C_l, C_h, [dst, #24]
++	ldrd	C_l, C_h, [src, #24]
++	strd	D_l, D_h, [dst, #32]
++	ldrd	D_l, D_h, [src, #32]
++	bcs	2b
++	/* Save the remaining bytes and restore the callee-saved regs.  */
++	strd	A_l, A_h, [dst, #40]
++	add	src, src, #40
++	strd	B_l, B_h, [dst, #48]
++	ldrd	B_l, B_h, [sp, #8]
++	strd	C_l, C_h, [dst, #56]
++	ldrd	C_l, C_h, [sp, #16]
++	strd	D_l, D_h, [dst, #64]
++	ldrd	D_l, D_h, [sp, #24]
++	add	dst, dst, #72
++	tst	tmp2, #0x3f
++	bne	L(tail63aligned)
++	ldr	tmp2, [sp], #FRAME_SIZE
++	bx	lr
++#endif
++
++L(cpy_notaligned):
++	pld	[src]
++	pld	[src, #64]
++	/* There's at least 64 bytes to copy, but there is no mutual
++	   alignment.  */
++	/* Bring DST to 64-bit alignment.  */
++	lsls	tmp2, dst, #29
++	pld	[src, #(2 * 64)]
++	beq	1f
++	rsbs	tmp2, tmp2, #0
++	sub	count, count, tmp2, lsr #29
++	ldrmi	tmp1, [src], #4
++	strmi	tmp1, [dst], #4
++	lsls	tmp2, tmp2, #2
++	ldrbne	tmp1, [src], #1
++	ldrhcs	tmp2, [src], #2
++	strbne	tmp1, [dst], #1
++	strhcs	tmp2, [dst], #2
++1:
++	pld	[src, #(3 * 64)]
++	subs	count, count, #64
++	ldrlo	tmp2, [sp], #FRAME_SIZE
++	blo	L(tail63unaligned)
++	pld	[src, #(4 * 64)]
++
++#ifdef USE_NEON
++	vld1.8	{d0-d3}, [src]!
++	vld1.8	{d4-d7}, [src]!
++	subs	count, count, #64
++	blo	2f
++1:
++	pld	[src, #(4 * 64)]
++	vst1.8	{d0-d3}, [ALIGN (dst, 64)]!
++	vld1.8	{d0-d3}, [src]!
++	vst1.8	{d4-d7}, [ALIGN (dst, 64)]!
++	vld1.8	{d4-d7}, [src]!
++	subs	count, count, #64
++	bhs	1b
++2:
++	vst1.8	{d0-d3}, [ALIGN (dst, 64)]!
++	vst1.8	{d4-d7}, [ALIGN (dst, 64)]!
++	ands	count, count, #0x3f
++#else
++	/* Use an SMS style loop to maximize the I/O bandwidth.  */
++	sub	src, src, #4
++	sub	dst, dst, #8
++	subs	tmp2, count, #64	/* Use tmp2 for count.  */
++	ldr	A_l, [src, #4]
++	ldr	A_h, [src, #8]
++	strd	B_l, B_h, [sp, #8]
++	ldr	B_l, [src, #12]
++	ldr	B_h, [src, #16]
++	strd	C_l, C_h, [sp, #16]
++	ldr	C_l, [src, #20]
++	ldr	C_h, [src, #24]
++	strd	D_l, D_h, [sp, #24]
++	ldr	D_l, [src, #28]
++	ldr	D_h, [src, #32]!
++	b	1f
++	.p2align	6
++2:
++	pld	[src, #(5 * 64) - (32 - 4)]
++	strd	A_l, A_h, [dst, #40]
++	ldr	A_l, [src, #36]
++	ldr	A_h, [src, #40]
++	strd	B_l, B_h, [dst, #48]
++	ldr	B_l, [src, #44]
++	ldr	B_h, [src, #48]
++	strd	C_l, C_h, [dst, #56]
++	ldr	C_l, [src, #52]
++	ldr	C_h, [src, #56]
++	strd	D_l, D_h, [dst, #64]!
++	ldr	D_l, [src, #60]
++	ldr	D_h, [src, #64]!
++	subs	tmp2, tmp2, #64
++1:
++	strd	A_l, A_h, [dst, #8]
++	ldr	A_l, [src, #4]
++	ldr	A_h, [src, #8]
++	strd	B_l, B_h, [dst, #16]
++	ldr	B_l, [src, #12]
++	ldr	B_h, [src, #16]
++	strd	C_l, C_h, [dst, #24]
++	ldr	C_l, [src, #20]
++	ldr	C_h, [src, #24]
++	strd	D_l, D_h, [dst, #32]
++	ldr	D_l, [src, #28]
++	ldr	D_h, [src, #32]
++	bcs	2b
++
++	/* Save the remaining bytes and restore the callee-saved regs.  */
++	strd	A_l, A_h, [dst, #40]
++	add	src, src, #36
++	strd	B_l, B_h, [dst, #48]
++	ldrd	B_l, B_h, [sp, #8]
++	strd	C_l, C_h, [dst, #56]
++	ldrd	C_l, C_h, [sp, #16]
++	strd	D_l, D_h, [dst, #64]
++	ldrd	D_l, D_h, [sp, #24]
++	add	dst, dst, #72
++	ands	count, tmp2, #0x3f
++#endif
++	ldr	tmp2, [sp], #FRAME_SIZE
++	bne	L(tail63unaligned)
++	bx	lr
++
+--- /dev/null	2024-10-19 22:20:54.374000100 +0800
++++ b/src/string/arm/memset.S	2024-10-25 00:14:08.667341353 +0800
+@@ -0,0 +1,97 @@
++/*
++ * memset - fill memory with a constant
++ *
++ * Copyright (c) 2010-2021, Arm Limited.
++ * SPDX-License-Identifier: MIT OR Apache-2.0 WITH LLVM-exception
++ */
++
++/*
++   Written by Dave Gilbert <david.gilbert@linaro.org>
++
++   This memset routine is optimised on a Cortex-A9 and should work on
++   all ARMv7 processors.
++
++ */
++
++	.syntax unified
++	.arch armv7-a
++
++@ 2011-08-30 david.gilbert@linaro.org
++@    Extracted from local git 2f11b436
++
++@ this lets us check a flag in a 00/ff byte easily in either endianness
++#ifdef __ARMEB__
++#define CHARTSTMASK(c) 1<<(31-(c*8))
++#else
++#define CHARTSTMASK(c) 1<<(c*8)
++#endif
++	.thumb
++
++@ ---------------------------------------------------------------------------
++	.thumb_func
++	.align 2
++	.p2align 4,,15
++.global memset
++.type memset,%function
++memset:
++	@ r0 = address
++	@ r1 = character
++	@ r2 = count
++	@ returns original address in r0
++
++	mov	r3, r0		@ Leave r0 alone
++	cbz	r2, 10f		@ Exit if 0 length
++
++	tst	r0, #7
++	beq	2f		@ Already aligned
++
++	@ Ok, so we're misaligned here
++1:
++	strb	r1, [r3], #1
++	subs	r2,r2,#1
++	tst	r3, #7
++	cbz	r2, 10f		@ Exit if we hit the end
++	bne	1b		@ go round again if still misaligned
++
++2:
++	@ OK, so we're aligned
++	push	{r4,r5,r6,r7}
++	bics	r4, r2, #15	@ if less than 16 bytes then need to finish it off
++	beq	5f
++
++3:
++	@ POSIX says that ch is cast to an unsigned char.  A uxtb is one
++	@ byte and takes two cycles, where an AND is four bytes but one
++	@ cycle.
++	and	r1, #0xFF
++	orr	r1, r1, r1, lsl#8	@ Same character into all bytes
++	orr	r1, r1, r1, lsl#16
++	mov	r5,r1
++	mov	r6,r1
++	mov	r7,r1
++
++4:
++	subs	r4,r4,#16
++	stmia	r3!,{r1,r5,r6,r7}
++	bne	4b
++	and	r2,r2,#15
++
++	@ At this point we're still aligned and we have upto align-1 bytes left to right
++	@ we can avoid some of the byte-at-a time now by testing for some big chunks
++	tst	r2,#8
++	itt	ne
++	subne	r2,r2,#8
++	stmiane	r3!,{r1,r5}
++
++5:
++	pop	{r4,r5,r6,r7}
++	cbz	r2, 10f
++
++	@ Got to do any last < alignment bytes
++6:
++	subs	r2,r2,#1
++	strb	r1,[r3],#1
++	bne	6b
++
++10:
++	bx	lr		@ goodbye
+--- /dev/null	2024-10-19 22:20:54.374000100 +0800
++++ b/include/asmdefs.h	2024-10-25 00:16:03.871885515 +0800
+@@ -0,0 +1,477 @@
++/*
++ * Macros for asm code.  Arm version.
++ *
++ * Copyright (c) 2019-2022, Arm Limited.
++ * SPDX-License-Identifier: MIT OR Apache-2.0 WITH LLVM-exception
++ */
++
++#ifndef _ASMDEFS_H
++#define _ASMDEFS_H
++
++/* Check whether leaf function PAC signing has been requested in the
++   -mbranch-protect compile-time option.  */
++#define LEAF_PROTECT_BIT 2
++
++#ifdef __ARM_FEATURE_PAC_DEFAULT
++# define HAVE_PAC_LEAF \
++	((__ARM_FEATURE_PAC_DEFAULT & (1 << LEAF_PROTECT_BIT)) && 1)
++#else
++# define HAVE_PAC_LEAF 0
++#endif
++
++/* Provide default parameters for PAC-code handling in leaf-functions.  */
++#if HAVE_PAC_LEAF
++# ifndef PAC_LEAF_PUSH_IP
++#  define PAC_LEAF_PUSH_IP 1
++# endif
++#else /* !HAVE_PAC_LEAF */
++# undef PAC_LEAF_PUSH_IP
++# define PAC_LEAF_PUSH_IP 0
++#endif /* HAVE_PAC_LEAF */
++
++#define STACK_ALIGN_ENFORCE 0
++
++/******************************************************************************
++* Implementation of the prologue and epilogue assembler macros and their
++* associated helper functions.
++*
++* These functions add support for the following:
++*
++* - M-profile branch target identification (BTI) landing-pads when compiled
++*   with `-mbranch-protection=bti'.
++* - PAC-signing and verification instructions, depending on hardware support
++*   and whether the PAC-signing of leaf functions has been requested via the
++*   `-mbranch-protection=pac-ret+leaf' compiler argument.
++* - 8-byte stack alignment preservation at function entry, defaulting to the
++*   value of STACK_ALIGN_ENFORCE.
++*
++* Notes:
++* - Prologue stack alignment is implemented by detecting a push with an odd
++*   number of registers and prepending a dummy register to the list.
++* - If alignment is attempted on a list containing r0, compilation will result
++*   in an error.
++* - If alignment is attempted in a list containing r1, r0 will be prepended to
++*   the register list and r0 will be restored prior to function return.  for
++*   functions with non-void return types, this will result in the corruption of
++*   the result register.
++* - Stack alignment is enforced via the following helper macro call-chain:
++*
++*	{prologue|epilogue} ->_align8 -> _preprocess_reglist ->
++*		_preprocess_reglist1 -> {_prologue|_epilogue}
++*
++* - Debug CFI directives are automatically added to prologues and epilogues,
++*   assisted by `cfisavelist' and `cfirestorelist', respectively.
++*
++* Arguments:
++* prologue
++* --------
++* - first	- If `last' specified, this serves as start of general-purpose
++*		  register (GPR) range to push onto stack, otherwise represents
++*		  single GPR to push onto stack.  If omitted, no GPRs pushed
++*		  onto stack at prologue.
++* - last	- If given, specifies inclusive upper-bound of GPR range.
++* - push_ip	- Determines whether IP register is to be pushed to stack at
++*		  prologue.  When pac-signing is requested, this holds the
++*		  the pac-key.  Either 1 or 0 to push or not push, respectively.
++*		  Default behavior: Set to value of PAC_LEAF_PUSH_IP macro.
++* - push_lr	- Determines whether to push lr to the stack on function entry.
++*		  Either 1 or 0  to push or not push, respectively.
++* - align8	- Whether to enforce alignment. Either 1 or 0, with 1 requesting
++*		  alignment.
++*
++* epilogue
++* --------
++*   The epilogue should be called passing the same arguments as those passed to
++*   the prologue to ensure the stack is not corrupted on function return.
++*
++* Usage examples:
++*
++*   prologue push_ip=1 -> push {ip}
++*   epilogue push_ip=1, align8=1 -> pop {r2, ip}
++*   prologue push_ip=1, push_lr=1 -> push {ip, lr}
++*   epilogue 1 -> pop {r1}
++*   prologue 1, align8=1 -> push {r0, r1}
++*   epilogue 1, push_ip=1 -> pop {r1, ip}
++*   prologue 1, 4 -> push {r1-r4}
++*   epilogue 1, 4 push_ip=1 -> pop {r1-r4, ip}
++*
++******************************************************************************/
++
++/* Emit .cfi_restore directives for a consecutive sequence of registers.  */
++	.macro cfirestorelist first, last
++	.cfi_restore \last
++	.if \last-\first
++	 cfirestorelist \first, \last-1
++	.endif
++	.endm
++
++/* Emit .cfi_offset directives for a consecutive sequence of registers.  */
++	.macro cfisavelist first, last, index=1
++	.cfi_offset \last, -4*(\index)
++	.if \last-\first
++	 cfisavelist \first, \last-1, \index+1
++	.endif
++	.endm
++
++.macro _prologue first=-1, last=-1, push_ip=PAC_LEAF_PUSH_IP, push_lr=0
++	.if \push_ip & 1 != \push_ip
++	 .error "push_ip may be either 0 or 1"
++	.endif
++	.if \push_lr & 1 != \push_lr
++	 .error "push_lr may be either 0 or 1"
++	.endif
++	.if \first != -1
++	 .if \last == -1
++	  /* Upper-bound not provided: Set upper = lower.  */
++	  _prologue \first, \first, \push_ip, \push_lr
++	  .exitm
++	 .endif
++	.endif
++#if HAVE_PAC_LEAF
++# if __ARM_FEATURE_BTI_DEFAULT
++	pacbti	ip, lr, sp
++# else
++	pac	ip, lr, sp
++# endif /* __ARM_FEATURE_BTI_DEFAULT */
++	.cfi_register 143, 12
++#else
++# if __ARM_FEATURE_BTI_DEFAULT
++	bti
++# endif /* __ARM_FEATURE_BTI_DEFAULT */
++#endif /* HAVE_PAC_LEAF */
++	.if \first != -1
++	 .if \last != \first
++	  .if \last >= 13
++	.error "SP cannot be in the save list"
++	  .endif
++	  .if \push_ip
++	   .if \push_lr
++	/* Case 1: push register range, ip and lr registers.  */
++	push {r\first-r\last, ip, lr}
++	.cfi_adjust_cfa_offset ((\last-\first)+3)*4
++	.cfi_offset 14, -4
++	.cfi_offset 143, -8
++	cfisavelist \first, \last, 3
++	   .else // !\push_lr
++	/* Case 2: push register range and ip register.  */
++	push {r\first-r\last, ip}
++	.cfi_adjust_cfa_offset ((\last-\first)+2)*4
++	.cfi_offset 143, -4
++	cfisavelist \first, \last, 2
++	   .endif
++	  .else // !\push_ip
++	   .if \push_lr
++	/* Case 3: push register range and lr register.  */
++	push {r\first-r\last, lr}
++	.cfi_adjust_cfa_offset ((\last-\first)+2)*4
++	.cfi_offset 14, -4
++	cfisavelist \first, \last, 2
++	   .else // !\push_lr
++	/* Case 4: push register range.  */
++	push {r\first-r\last}
++	.cfi_adjust_cfa_offset ((\last-\first)+1)*4
++	cfisavelist \first, \last, 1
++	   .endif
++	  .endif
++	 .else // \last == \first
++	  .if \push_ip
++	   .if \push_lr
++	/* Case 5: push single GP register plus ip and lr registers.  */
++	push {r\first, ip, lr}
++	.cfi_adjust_cfa_offset 12
++	.cfi_offset 14, -4
++	.cfi_offset 143, -8
++        cfisavelist \first, \first, 3
++	   .else // !\push_lr
++	/* Case 6: push single GP register plus ip register.  */
++	push {r\first, ip}
++	.cfi_adjust_cfa_offset 8
++	.cfi_offset 143, -4
++        cfisavelist \first, \first, 2
++	   .endif
++	  .else // !\push_ip
++	   .if \push_lr
++	/* Case 7: push single GP register plus lr register.  */
++	push {r\first, lr}
++	.cfi_adjust_cfa_offset 8
++	.cfi_offset 14, -4
++	cfisavelist \first, \first, 2
++	   .else // !\push_lr
++	/* Case 8: push single GP register.  */
++	push {r\first}
++	.cfi_adjust_cfa_offset 4
++	cfisavelist \first, \first, 1
++	   .endif
++	  .endif
++	 .endif
++	.else // \first == -1
++	 .if \push_ip
++	  .if \push_lr
++	/* Case 9: push ip and lr registers.  */
++	push {ip, lr}
++	.cfi_adjust_cfa_offset 8
++	.cfi_offset 14, -4
++	.cfi_offset 143, -8
++	  .else // !\push_lr
++	/* Case 10: push ip register.  */
++	push {ip}
++	.cfi_adjust_cfa_offset 4
++	.cfi_offset 143, -4
++	  .endif
++	 .else // !\push_ip
++          .if \push_lr
++	/* Case 11: push lr register.  */
++	push {lr}
++	.cfi_adjust_cfa_offset 4
++	.cfi_offset 14, -4
++          .endif
++	 .endif
++	.endif
++.endm
++
++.macro _epilogue first=-1, last=-1, push_ip=PAC_LEAF_PUSH_IP, push_lr=0
++	.if \push_ip & 1 != \push_ip
++	 .error "push_ip may be either 0 or 1"
++	.endif
++	.if \push_lr & 1 != \push_lr
++	 .error "push_lr may be either 0 or 1"
++	.endif
++	.if \first != -1
++	 .if \last == -1
++	  /* Upper-bound not provided: Set upper = lower.  */
++	  _epilogue \first, \first, \push_ip, \push_lr
++	  .exitm
++	 .endif
++	 .if \last != \first
++	  .if \last >= 13
++	.error "SP cannot be in the save list"
++	  .endif
++	  .if \push_ip
++	   .if \push_lr
++	/* Case 1: pop register range, ip and lr registers.  */
++	pop {r\first-r\last, ip, lr}
++	.cfi_restore 14
++	.cfi_register 143, 12
++	cfirestorelist \first, \last
++	   .else // !\push_lr
++	/* Case 2: pop register range and ip register.  */
++	pop {r\first-r\last, ip}
++	.cfi_register 143, 12
++	cfirestorelist \first, \last
++	   .endif
++	  .else // !\push_ip
++	   .if \push_lr
++	/* Case 3: pop register range and lr register.  */
++	pop {r\first-r\last, lr}
++	.cfi_restore 14
++	cfirestorelist \first, \last
++	   .else // !\push_lr
++	/* Case 4: pop register range.  */
++	pop {r\first-r\last}
++	cfirestorelist \first, \last
++	   .endif
++	  .endif
++	 .else // \last == \first
++	  .if \push_ip
++	   .if \push_lr
++	/* Case 5: pop single GP register plus ip and lr registers.  */
++	pop {r\first, ip, lr}
++	.cfi_restore 14
++	.cfi_register 143, 12
++	cfirestorelist \first, \first
++	   .else // !\push_lr
++	/* Case 6: pop single GP register plus ip register.  */
++	pop {r\first, ip}
++	.cfi_register 143, 12
++	cfirestorelist \first, \first
++	   .endif
++	  .else // !\push_ip
++	   .if \push_lr
++	/* Case 7: pop single GP register plus lr register.  */
++	pop {r\first, lr}
++	.cfi_restore 14
++	cfirestorelist \first, \first
++	   .else // !\push_lr
++	/* Case 8: pop single GP register.  */
++	pop {r\first}
++	cfirestorelist \first, \first
++	   .endif
++	  .endif
++	 .endif
++	.else // \first == -1
++	 .if \push_ip
++	  .if \push_lr
++	/* Case 9: pop ip and lr registers.  */
++	pop {ip, lr}
++	.cfi_restore 14
++	.cfi_register 143, 12
++	  .else // !\push_lr
++	/* Case 10: pop ip register.  */
++	pop {ip}
++	.cfi_register 143, 12
++	  .endif
++	 .else // !\push_ip
++          .if \push_lr
++	/* Case 11: pop lr register.  */
++	pop {lr}
++	.cfi_restore 14
++          .endif
++	 .endif
++	.endif
++#if HAVE_PAC_LEAF
++	aut	ip, lr, sp
++#endif /* HAVE_PAC_LEAF */
++	bx	lr
++.endm
++
++/* Clean up expressions in 'last'.  */
++.macro _preprocess_reglist1 first:req, last:req, push_ip:req, push_lr:req, reglist_op:req
++	.if \last == 0
++	 \reglist_op \first, 0, \push_ip, \push_lr
++	.elseif \last == 1
++	 \reglist_op \first, 1, \push_ip, \push_lr
++	.elseif \last == 2
++	 \reglist_op \first, 2, \push_ip, \push_lr
++	.elseif \last == 3
++	 \reglist_op \first, 3, \push_ip, \push_lr
++	.elseif \last == 4
++	 \reglist_op \first, 4, \push_ip, \push_lr
++	.elseif \last == 5
++	 \reglist_op \first, 5, \push_ip, \push_lr
++	.elseif \last == 6
++	 \reglist_op \first, 6, \push_ip, \push_lr
++	.elseif \last == 7
++	 \reglist_op \first, 7, \push_ip, \push_lr
++	.elseif \last == 8
++	 \reglist_op \first, 8, \push_ip, \push_lr
++	.elseif \last == 9
++	 \reglist_op \first, 9, \push_ip, \push_lr
++	.elseif \last == 10
++	 \reglist_op \first, 10, \push_ip, \push_lr
++	.elseif \last == 11
++	 \reglist_op \first, 11, \push_ip, \push_lr
++	.else
++	 .error "last (\last) out of range"
++	.endif
++.endm
++
++/* Clean up expressions in 'first'.  */
++.macro _preprocess_reglist first:req, last, push_ip=0, push_lr=0, reglist_op:req
++	.ifb \last
++	 _preprocess_reglist \first \first \push_ip \push_lr
++	.else
++	 .if \first > \last
++	  .error "last (\last) must be at least as great as first (\first)"
++	 .endif
++	 .if \first == 0
++	  _preprocess_reglist1 0, \last, \push_ip, \push_lr, \reglist_op
++	 .elseif \first == 1
++	  _preprocess_reglist1 1, \last, \push_ip, \push_lr, \reglist_op
++	 .elseif \first == 2
++	  _preprocess_reglist1 2, \last, \push_ip, \push_lr, \reglist_op
++	 .elseif \first == 3
++	  _preprocess_reglist1 3, \last, \push_ip, \push_lr, \reglist_op
++	 .elseif \first == 4
++	  _preprocess_reglist1 4, \last, \push_ip, \push_lr, \reglist_op
++	 .elseif \first == 5
++	  _preprocess_reglist1 5, \last, \push_ip, \push_lr, \reglist_op
++	 .elseif \first == 6
++	  _preprocess_reglist1 6, \last, \push_ip, \push_lr, \reglist_op
++	 .elseif \first == 7
++	  _preprocess_reglist1 7, \last, \push_ip, \push_lr, \reglist_op
++	 .elseif \first == 8
++	  _preprocess_reglist1 8, \last, \push_ip, \push_lr, \reglist_op
++	 .elseif \first == 9
++	  _preprocess_reglist1 9, \last, \push_ip, \push_lr, \reglist_op
++	 .elseif \first == 10
++	  _preprocess_reglist1 10, \last, \push_ip, \push_lr, \reglist_op
++	 .elseif \first == 11
++	  _preprocess_reglist1 11, \last, \push_ip, \push_lr, \reglist_op
++	 .else
++	  .error "first (\first) out of range"
++	 .endif
++	.endif
++.endm
++
++.macro _align8 first, last, push_ip=0, push_lr=0, reglist_op=_prologue
++	.ifb \first
++	 .ifnb \last
++	  .error "can't have last (\last) without specifying first"
++	 .else // \last not blank
++	  .if ((\push_ip + \push_lr) % 2) == 0
++	   \reglist_op first=-1, last=-1, push_ip=\push_ip, push_lr=\push_lr
++	   .exitm
++	  .else // ((\push_ip + \push_lr) % 2) odd
++	   _align8 2, 2, \push_ip, \push_lr, \reglist_op
++	   .exitm
++	  .endif // ((\push_ip + \push_lr) % 2) == 0
++	 .endif // .ifnb \last
++	.endif // .ifb \first
++
++	.ifb \last
++	 _align8 \first, \first, \push_ip, \push_lr, \reglist_op
++	.else
++	 .if \push_ip & 1 <> \push_ip
++	  .error "push_ip may be 0 or 1"
++	 .endif
++	 .if \push_lr & 1 <> \push_lr
++	  .error "push_lr may be 0 or 1"
++	 .endif
++	 .ifeq (\last - \first + \push_ip + \push_lr) % 2
++	  .if \first == 0
++	   .error "Alignment required and first register is r0"
++	   .exitm
++	  .endif
++	  _preprocess_reglist \first-1, \last, \push_ip, \push_lr, \reglist_op
++	 .else
++	  _preprocess_reglist \first \last, \push_ip, \push_lr, \reglist_op
++	 .endif
++	.endif
++.endm
++
++.macro prologue first, last, push_ip=PAC_LEAF_PUSH_IP, push_lr=0, align8=STACK_ALIGN_ENFORCE
++	.if \align8
++	 _align8 \first, \last, \push_ip, \push_lr, _prologue
++	.else
++	 _prologue first=\first, last=\last, push_ip=\push_ip, push_lr=\push_lr
++	.endif
++.endm
++
++.macro epilogue first, last, push_ip=PAC_LEAF_PUSH_IP, push_lr=0, align8=STACK_ALIGN_ENFORCE
++	.if \align8
++	 _align8 \first, \last, \push_ip, \push_lr, reglist_op=_epilogue
++	.else
++	 _epilogue first=\first, last=\last, push_ip=\push_ip, push_lr=\push_lr
++	.endif
++.endm
++
++#define ENTRY_ALIGN(name, alignment)	\
++  .global name;		\
++  .type name,%function;	\
++  .align alignment;		\
++  name:			\
++  .fnstart;		\
++  .cfi_startproc;
++
++#define ENTRY(name)	ENTRY_ALIGN(name, 6)
++
++#define ENTRY_ALIAS(name)	\
++  .global name;		\
++  .type name,%function;	\
++  name:
++
++#if defined (IS_LEAF)
++# define END_UNWIND .cantunwind;
++#else
++# define END_UNWIND
++#endif
++
++#define END(name)	\
++  .cfi_endproc;		\
++  END_UNWIND		\
++  .fnend;		\
++  .size name, .-name;
++
++#define L(l) .L ## l
++
++#endif
+
